{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transparency ML in Text Classification using IMDB Reviews (large) datasets\n",
    "\n",
    "The source of this dataset can be accessed through the following link: http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "This dataset provides data for train and test. Each of dataset contains 25,000 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Compulsory Standard Library #################\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "############ Utility Library #################\n",
    "from prettytable import PrettyTable\n",
    "import xlsxwriter\n",
    "import time\n",
    "import graphviz\n",
    "\n",
    "############ Sklearn pre-processing Library #################\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split, ShuffleSplit\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "############ Sklearn model Library #################\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_imdb(path):\n",
    "    \n",
    "    print(\"Loading the imdb data\")\n",
    "    \n",
    "    train_neg_files = glob.glob(path+\"/train/neg/*.txt\")\n",
    "    train_pos_files = glob.glob(path+\"/train/pos/*.txt\")\n",
    "    \n",
    "    X_train_corpus = []\n",
    "    y_train = []\n",
    "    \n",
    "    for tnf in train_neg_files:\n",
    "        f = open(tnf, 'r', encoding=\"utf8\")\n",
    "        line = f.read()\n",
    "        #line = line[:len(line)/2]\n",
    "        X_train_corpus.append(line)\n",
    "        y_train.append(0)\n",
    "        f.close()\n",
    "    \n",
    "    for tpf in train_pos_files:\n",
    "        f = open(tpf, 'r', encoding=\"utf8\")\n",
    "        line = f.read()\n",
    "        #line = line[:len(line)/2]\n",
    "        X_train_corpus.append(line)\n",
    "        y_train.append(1)\n",
    "        f.close()\n",
    "    \n",
    "    print(\"Train Data loaded.\")\n",
    "    \n",
    "    test_neg_files = glob.glob(path+\"/test/neg/*.txt\")\n",
    "    test_pos_files = glob.glob(path+\"/test/pos/*.txt\")\n",
    "    \n",
    "    X_test_corpus = []\n",
    "    y_test = []\n",
    "    \n",
    "    for tnf in test_neg_files:\n",
    "        f = open(tnf, 'r', encoding=\"utf8\")\n",
    "        X_test_corpus.append(f.read())\n",
    "        y_test.append(0)\n",
    "        f.close()\n",
    "    \n",
    "    for tpf in test_pos_files:\n",
    "        f = open(tpf, 'r', encoding=\"utf8\")\n",
    "        X_test_corpus.append(f.read())\n",
    "        y_test.append(1)\n",
    "        f.close()\n",
    "    \n",
    "    print(\"Test Data loaded.\")\n",
    "    \n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    return X_train_corpus, y_train, X_test_corpus , y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the imdb data\n",
      "Train Data loaded.\n",
      "Test Data loaded.\n"
     ]
    }
   ],
   "source": [
    "X_train_corpus , y_train, X_test_corpus , y_test = load_imdb('../../aclImdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Vectorizer Transform start\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "tf_vectorizer = CountVectorizer(lowercase=True, max_df=1.0, min_df=5, binary=True, ngram_range=(2,2))\n",
    "\n",
    "print('Data Vectorizer Transform start')\n",
    "print()\n",
    "X_train = tf_vectorizer.fit_transform(X_train_corpus)\n",
    "\n",
    "# print('Train Data Transformed')\n",
    "# print('Train Data size ', X_train.shape)\n",
    "# print()\n",
    "# X_test = tf_vectorizer.transform(X_test_corpus)\n",
    "# print('Test Data Transformed')\n",
    "# print('Test Data size ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129549"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = tf_vectorizer.get_feature_names()\n",
    "len(tf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf_vectorizer.stop_words_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Not\" and \"Nothing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n"
     ]
    }
   ],
   "source": [
    "voc_nothing = [s for s in words if \"nothing\" in s]\n",
    "print(len(voc_nothing))\n",
    "# print(voc_nothing)\n",
    "\n",
    "with open('nothing-2grams.txt', mode='w', encoding='utf8') as w:\n",
    "        for i in voc_nothing:\n",
    "            w.write(i)\n",
    "            w.write('\\n')\n",
    "        w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343\n"
     ]
    }
   ],
   "source": [
    "voc_never = [s for s in words if \"never\" in s]\n",
    "print(len(voc_never))\n",
    "\n",
    "with open('never-2grams.txt', mode='w', encoding='utf8') as w:\n",
    "        for i in voc_never:\n",
    "            w.write(i)\n",
    "            w.write('\\n')\n",
    "        w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "663\n"
     ]
    }
   ],
   "source": [
    "voc_not = [s for s in words if \"not \" in s]\n",
    "print(len(voc_not))\n",
    "\n",
    "with open('not-2grams.txt', mode='w', encoding='utf8') as w:\n",
    "        for i in voc_not:\n",
    "            w.write(i)\n",
    "            w.write('\\n')\n",
    "        w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "voc_slightly = [s for s in words if \"slightly\" in s]\n",
    "print(len(voc_slightly))\n",
    "\n",
    "with open('slightly-2grams.txt', mode='w', encoding='utf8') as w:\n",
    "        for i in voc_slightly:\n",
    "            w.write(i)\n",
    "            w.write('\\n')\n",
    "        w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "voc_may = [s for s in words if \"may \" in s]\n",
    "print(len(voc_may))\n",
    "\n",
    "with open('may-2grams.txt', mode='w', encoding='utf8') as w:\n",
    "        for i in voc_may:\n",
    "            w.write(i)\n",
    "            w.write('\\n')\n",
    "        w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "voc_maybe = [s for s in words if \"maybe\" in s]\n",
    "print(len(voc_maybe))\n",
    "\n",
    "with open('maybe-2grams.txt', mode='w', encoding='utf8') as w:\n",
    "        for i in voc_maybe:\n",
    "            w.write(i)\n",
    "            w.write('\\n')\n",
    "        w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n"
     ]
    }
   ],
   "source": [
    "voc_always = [s for s in words if \"always\" in s]\n",
    "print(len(voc_always))\n",
    "\n",
    "with open('always-2grams.txt', mode='w', encoding='utf8') as w:\n",
    "        for i in voc_always:\n",
    "            w.write(i)\n",
    "            w.write('\\n')\n",
    "        w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n"
     ]
    }
   ],
   "source": [
    "voc_almost = [s for s in words if \"almost\" in s]\n",
    "print(len(voc_almost))\n",
    "\n",
    "with open('almost-2grams.txt', mode='w', encoding='utf8') as w:\n",
    "        for i in voc_almost:\n",
    "            w.write(i)\n",
    "            w.write('\\n')\n",
    "        w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, max_df=1.0, min_df=5, binary=True, ngram_range=(2,2), vocabulary=voc_nothing)\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train_corpus)\n",
    "X_test = vectorizer.transform(X_test_corpus)\n",
    "\n",
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_l2 = LogisticRegression(penalty='l2', C=0.5)\n",
    "clf_l2.fit(X_train, y_train)\n",
    "print(clf_noz_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_1 = clf_l2.predict(X_train)\n",
    "print(accuracy_score(y_pred_1, y_train))\n",
    "y_pred_2 = clf_l2.predict(X_test)\n",
    "print(accuracy_score(y_pred_2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_l1 = LogisticRegression(penalty='l1')\n",
    "clf_l1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_1 = clf_l1.predict(X_train)\n",
    "print(accuracy_score(y_pred_1, y_train))\n",
    "y_pred_2 = clf_l1.predict(X_test)\n",
    "print(accuracy_score(y_pred_2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_features(clf_L1, clf_L2, f_names, msg, iter_range=10):\n",
    "    idx_L2 = np.argsort(np.absolute(clf_L2.coef_)[0,:])[::-1]\n",
    "    idx_L1 = np.argsort(np.absolute(clf_L1.coef_)[0,:])[::-1]\n",
    "    \n",
    "    ### Print on Pretty Table\n",
    "    table_features = PrettyTable(['Rank', 'L2 Features', 'L2 Weight', 'L1 Features', 'L1 Weight'])\n",
    "    f_list_1 = np.zeros(iter_range, dtype='int16')\n",
    "    f_list_2 = np.ones(iter_range, dtype='int16')\n",
    "    \n",
    "    f_list = []\n",
    "    \n",
    "    for idx in range(0,iter_range):\n",
    "        table_features.add_row([idx+1, \n",
    "                                words[idx_L2[idx]], \n",
    "                                np.around(clf_L2.coef_[0,idx_L2[idx]], decimals=4), \n",
    "                                words[idx_L1[idx]], \n",
    "                                np.around(clf_L1.coef_[0,idx_L1[idx]], decimals=4)])\n",
    "        f_list_2[idx] = idx_L2[idx]\n",
    "        f_list_1[idx] = idx_L1[idx]\n",
    "        \n",
    "    print('L2 and L1-regularized Logistic Regression Classifier', msg)\n",
    "    print('Top 10 features and weights (with absolute value)')\n",
    "    print()\n",
    "    print(table_features)\n",
    "    print(' ')\n",
    "    print('List of features in both L1 and L2 penalty :')\n",
    "    num=1\n",
    "    for i in range(0,iter_range):\n",
    "        for j in range(0,iter_range):\n",
    "            if f_list_1[i] == f_list_2[j]:\n",
    "                f_list.append(words[f_list_1[i]])\n",
    "                #print('\\t', num, f_names[f_list_1[i]])\n",
    "                num += 1\n",
    "                \n",
    "    for v in f_list:\n",
    "        print(v)\n",
    "    print(num-1, 'Similar Features')\n",
    "    \n",
    "    return f_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_noz_list = print_features(clf_l1, clf_l2, f_names, 'without Z-score scaling', iter_range=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "freq = np.sum(X_train, axis=0)\n",
    "freq = freq.A1\n",
    "weights = clf_l2.coef_[0]\n",
    "fi = np.argsort(np.absolute(freq))\n",
    "\n",
    "[(words[i], freq[i], weights[i]) for i in fi[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "print(colored('hello', 'red'), colored('world', 'green'))\n",
    "print(colored(\"hello red world\", 'red'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
