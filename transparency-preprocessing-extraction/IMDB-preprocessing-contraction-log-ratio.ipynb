{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############ Compulsory Standard Library #################\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import graphviz\n",
    "from anytree import Node, RenderTree\n",
    "\n",
    "############ Sklearn pre-processing Library #################\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "############ Sklearn model Library #################\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_imdb(path):\n",
    "    \n",
    "    print(\"Loading the imdb data\")\n",
    "    \n",
    "    train_neg_files = glob.glob(path+\"/train/neg/*.txt\")\n",
    "    train_pos_files = glob.glob(path+\"/train/pos/*.txt\")\n",
    "    \n",
    "    X_train_corpus = []\n",
    "    y_train = []\n",
    "    \n",
    "    for tnf in train_neg_files:\n",
    "        f = open(tnf, 'r', encoding=\"utf8\")\n",
    "        line = f.read()\n",
    "        #line = line[:len(line)/2]\n",
    "        X_train_corpus.append(line)\n",
    "        y_train.append(0)\n",
    "        f.close()\n",
    "    \n",
    "    for tpf in train_pos_files:\n",
    "        f = open(tpf, 'r', encoding=\"utf8\")\n",
    "        line = f.read()\n",
    "        #line = line[:len(line)/2]\n",
    "        X_train_corpus.append(line)\n",
    "        y_train.append(1)\n",
    "        f.close()\n",
    "    \n",
    "    print(\"Train Data loaded.\")\n",
    "    \n",
    "    test_neg_files = glob.glob(path+\"/test/neg/*.txt\")\n",
    "    test_pos_files = glob.glob(path+\"/test/pos/*.txt\")\n",
    "    \n",
    "    X_test_corpus = []\n",
    "    y_test = []\n",
    "    \n",
    "    for tnf in test_neg_files:\n",
    "        f = open(tnf, 'r', encoding=\"utf8\")\n",
    "        X_test_corpus.append(f.read())\n",
    "        y_test.append(0)\n",
    "        f.close()\n",
    "    \n",
    "    for tpf in test_pos_files:\n",
    "        f = open(tpf, 'r', encoding=\"utf8\")\n",
    "        X_test_corpus.append(f.read())\n",
    "        y_test.append(1)\n",
    "        f.close()\n",
    "    \n",
    "    print(\"Test Data loaded.\")\n",
    "    \n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    return X_train_corpus, y_train, X_test_corpus , y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_list(filename, split_delimiter):\n",
    "    vocabulary = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for l in f:\n",
    "            vocabulary.append(l.strip().split(split_delimiter))\n",
    "    return np.asarray(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_contraction(corpus, cont_list):\n",
    "    for i in range(0, cont_list.shape[0]):\n",
    "        corpus = corpus.lower().replace(cont_list[i,0], cont_list[i,1])\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_corpus_contraction(X_corpus):\n",
    "    cont_list = load_list(\"contraction_list.txt\", ',')\n",
    "    print(cont_list.shape)\n",
    "    print('corpus update start')\n",
    "    for i in range(0,len(X_corpus)):\n",
    "        X_corpus[i] = cleanhtml(X_corpus[i])\n",
    "        X_corpus[i] = replace_contraction(X_corpus[i], cont_list)\n",
    "    print('corpus update end')\n",
    "    print()\n",
    "    return X_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negative_positive_counts(X, y, word_index):\n",
    "    neg_count = np.sum(X[y==0, word_index])\n",
    "    pos_count = np.sum(X[y==1, word_index])    \n",
    "    return neg_count, pos_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_ratio_positive_negative(X, y, word_index):\n",
    "    neg_count, pos_count = negative_positive_counts(X,y, word_index)\n",
    "    log_ratio = np.log(pos_count)-np.log(neg_count)\n",
    "    return log_ratio, neg_count, pos_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_top_words_with_count(X, y, words,filename, top_k=10):\n",
    "    log_ratio = []\n",
    "    neg_count = []\n",
    "    pos_count = []\n",
    "    \n",
    "    for i in range(0,len(words)):\n",
    "        log_ratio_, neg_count_, pos_count_ = log_ratio_positive_negative(X, y, i)\n",
    "        log_ratio.append(log_ratio_)\n",
    "        neg_count.append(neg_count_)\n",
    "        pos_count.append(pos_count_)\n",
    "    \n",
    "    sorted_indices_descending_abs = np.argsort(np.absolute(log_ratio))[::-1]\n",
    "    \n",
    "    filename = filename + '.txt'\n",
    "    with open(filename, mode='w', encoding='utf8') as w:\n",
    "        for i in sorted_indices_descending_abs[: top_k]:\n",
    "#             print(\"%s\\t%0.2f\" %(words[i], weights[i]))\n",
    "#             n_p=negative_positive_counts(X, y, i)\n",
    "            w.write(\"%s\\t%0.2f\\t%d\\t%d\" %(str(words[i]), log_ratio[i], neg_count[i], pos_count[i]))\n",
    "            w.write('\\n')\n",
    "        w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanhtml(text):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantag = re.sub(cleanr, '', text)\n",
    "    cleantext = cleantag.replace('br', '')\n",
    "    return cleantext\n",
    "\n",
    "# Reference :\n",
    "# https://stackoverflow.com/questions/9662346/python-code-to-remove-html-tags-from-a-string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the imdb data\n",
      "Train Data loaded.\n",
      "Test Data loaded.\n"
     ]
    }
   ],
   "source": [
    "X_train_corpus , y_train, X_test_corpus , y_test = load_imdb('../aclImdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Update (Remove the contraction)\n",
    "Such as :<br>\n",
    "[isn't $\\rightarrow$ is not] <br>\n",
    "[haven't $\\rightarrow$ have not] <br>\n",
    "ain't -> are not (despite it also could be a 'is not' or 'am not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 2)\n",
      "corpus update start\n",
      "corpus update end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_corpus_update = update_corpus_contraction(X_train_corpus)\n",
    "# X_test_corpus_update = update_corpus_contraction(X_test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top 100 words extraction (1-5 grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token = r\"(?u)\\b[\\w\\'/]+\\b\"\n",
    "tf_vectorizer = CountVectorizer(lowercase=True, max_df=1.0, min_df=100, binary=True, token_pattern=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_vectorizer.set_params(ngram_range=(1,1))\n",
    "X_train = tf_vectorizer.fit_transform(X_train_corpus_update)\n",
    "words = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3851"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_top_words_with_count(X_train, y_train, words, '1gram', top_k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 grams\n",
      "Processing 2 grams\n",
      "Processing 3 grams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anne Soraya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 4 grams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anne Soraya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 5 grams\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    print('Processing',i,'grams')\n",
    "    tf_vectorizer.set_params(ngram_range=(i,i))\n",
    "    X_train = tf_vectorizer.fit_transform(X_train_corpus_update)\n",
    "    words = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "#     clf = LogisticRegression(penalty='l2', C=1, random_state=42)\n",
    "#     clf.fit(X_train, y_train)\n",
    "\n",
    "    filename=str(i)+\"gram\"\n",
    "    sort_top_words_with_count(X_train, y_train, words, filename, top_k=100)\n",
    "    \n",
    "#     del clf\n",
    "    del X_train\n",
    "    del words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate tree relation\n",
    "\n",
    "Reference : <br>\n",
    "https://stackoverflow.com/questions/2358045/how-can-i-implement-a-tree-in-python-are-there-any-built-in-data-structures-in\n",
    "http://anytree.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_gram = load_list(\"1gram.txt\",'\\t')\n",
    "two_gram = load_list(\"2gram.txt\",'\\t')\n",
    "three_gram = load_list(\"3gram.txt\", '\\t')\n",
    "four_gram = load_list(\"4gram.txt\", '\\t')\n",
    "five_gram = load_list(\"5gram.txt\", '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi my friend\n"
     ]
    }
   ],
   "source": [
    "string_one = \"hi\"\n",
    "string_two = \"my\"\n",
    "string_three = \"friend\"\n",
    "print(string_one + \" \" + string_two + \" \"+ string_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waste -2.55 \t waste your -3.39\n",
      "waste -2.55 \t waste of -3.05\n",
      "waste -2.55 \t not waste -2.97\n",
      "waste -2.55 \t a waste -2.50\n",
      "waste -2.55 \t to waste -2.00\n",
      "worst -2.20 \t worst films -4.65\n",
      "worst -2.20 \t worst movies -3.90\n",
      "worst -2.20 \t worst film -3.63\n",
      "worst -2.20 \t worst movie -3.41\n",
      "worst -2.20 \t the worst -2.32\n",
      "worst -2.20 \t worst of -1.96\n",
      "awful -2.14 \t awful the -3.30\n",
      "awful -2.14 \t is awful -3.06\n",
      "awful -2.14 \t was awful -3.02\n",
      "awful -2.14 \t awful i -2.95\n",
      "awful -2.14 \t awful and -2.65\n",
      "poorly -2.11 \t poorly written -2.65\n",
      "captures 2.06 \t captures the 2.65\n",
      "existent -2.04 \t non existent -2.01\n",
      "insult -1.94 \t an insult -2.36\n",
      "insult -1.94 \t insult to -2.29\n",
      "crap -1.78 \t this crap -4.26\n",
      "crap -1.78 \t of crap -2.43\n",
      "mess -1.72 \t this mess -3.65\n",
      "horrible -1.71 \t is horrible -2.22\n",
      "superb 1.69 \t is superb 1.99\n",
      "superb 1.69 \t a superb 1.88\n",
      "pile -1.65 \t pile of -2.05\n",
      "terrible -1.64 \t was terrible -2.75\n",
      "terrible -1.64 \t terrible the -2.61\n",
      "terrible -1.64 \t is terrible -2.47\n",
      "worse -1.61 \t is worse -3.01\n",
      "worse -1.61 \t even worse -2.44\n",
      "worse -1.61 \t worse than -2.12\n",
      "stupid -1.55 \t stupid and -1.84\n",
      "avoid -1.54 \t avoid this -3.05\n",
      "pleasantly 1.51 \t pleasantly surprised 1.95\n",
      "excuse -1.49 \t excuse for -2.00\n",
      "\n",
      "negated\n",
      "['a' 'non' 'of' 'to']\n",
      "amplifier\n",
      "['a' 'an' 'and' 'even' 'film' 'films' 'for' 'i' 'is' 'movie' 'movies' 'not'\n",
      " 'of' 'surprised' 'than' 'the' 'this' 'to' 'was' 'written' 'your']\n"
     ]
    }
   ],
   "source": [
    "negated = []\n",
    "amplifier = []\n",
    "ind_words = 0\n",
    "for i in one_gram:\n",
    "    for j in two_gram:\n",
    "        split_words = j[0].split()\n",
    "        after_weight = float(j[1])\n",
    "        previous_weight = float(i[1])\n",
    "        for k in range(0, len(split_words)):\n",
    "            if i[0] == split_words[k]:\n",
    "                if k == 0:\n",
    "                    ind_words = 1\n",
    "                elif k == 1:\n",
    "                    ind_words = 0\n",
    "                   \n",
    "                if np.sign(after_weight)!=np.sign(previous_weight) or np.absolute(after_weight) < np.absolute(previous_weight):\n",
    "                    negated.append(split_words[ind_words])\n",
    "                else:\n",
    "                    amplifier.append(split_words[ind_words])\n",
    "                print(i[0], i[1],'\\t', j[0], j[1])\n",
    "print()\n",
    "print('negated')\n",
    "print(np.unique(negated))\n",
    "print('amplifier')\n",
    "print(np.unique(amplifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negated = []\n",
    "amplifier = []\n",
    "ind_words = 0\n",
    "for i in one_gram:\n",
    "    for j in three_gram:\n",
    "        split_words = j[0].split()\n",
    "        after_weight = float(j[1])\n",
    "        previous_weight = float(i[1])\n",
    "        for k in range(0, len(split_words)):\n",
    "            if i[0] == split_words[k] and (np.absolute(after_weight)>np.absolute(previous_weight) or np.sign(after_weight)!=np.sign(previous_weight)):\n",
    "                if k == 0:\n",
    "                    ind_words = 1\n",
    "                elif k == 1:\n",
    "                    ind_words = 0\n",
    "                   \n",
    "                if np.sign(after_weight)!=np.sign(previous_weight):\n",
    "                    negated.append(split_words[ind_words])\n",
    "                else:\n",
    "                    amplifier.append(split_words[ind_words])\n",
    "                print(i[0], i[1],'\\t', j[0], j[1])\n",
    "print()\n",
    "print(negated)\n",
    "print(amplifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst movies -3.90 \t worst movies i -inf\n",
      "worst movies -3.90 \t the worst movies -4.41\n",
      "worst film -3.63 \t the worst film -3.96\n",
      "worst movie -3.41 \t worst movies i -inf\n",
      "worst movie -3.41 \t the worst movies -4.41\n",
      "worst movie -3.41 \t worst movie i -4.07\n",
      "worst movie -3.41 \t the worst movie -3.66\n",
      "waste your -3.39 \t not waste your -3.53\n",
      "not waste -2.97 \t not waste your -3.53\n",
      "not waste -2.97 \t do not waste -3.30\n",
      "all costs -2.60 \t at all costs -2.77\n",
      "a waste -2.50 \t a waste of -2.81\n",
      "only good -2.48 \t the only good -2.91\n",
      "loved this 2.45 \t i loved this 3.03\n",
      "of crap -2.43 \t piece of crap -3.75\n",
      "not worth -2.34 \t is not worth -2.63\n",
      "this piece -2.33 \t this piece of -2.83\n",
      "the worst -2.32 \t the worst movies -4.41\n",
      "the worst -2.32 \t the worst film -3.96\n",
      "the worst -2.32 \t the worst movie -3.66\n",
      "the worst -2.32 \t of the worst -3.24\n",
      "the worst -2.32 \t is the worst -2.70\n",
      "a must 2.19 \t is a must 2.30\n",
      "a must 2.19 \t a must see 2.28\n",
      "so bad -2.14 \t was so bad -2.99\n",
      "so bad -2.14 \t is so bad -2.72\n",
      "so bad -2.14 \t so bad that -2.36\n",
      "so bad -2.14 \t so bad it -2.25\n",
      "highly recommend 2.05 \t highly recommend this 2.81\n",
      "highly recommend 2.05 \t i highly recommend 2.09\n",
      "your time -1.98 \t waste your time -3.28\n",
      "loved it 1.95 \t i loved it 2.40\n",
      "bad i -1.91 \t so bad it -2.25\n",
      "bad i -1.91 \t bad it is -1.93\n",
      "must see 1.84 \t must see for 2.80\n",
      "must see 1.84 \t a must see 2.28\n",
      "bad it -1.83 \t so bad it -2.25\n",
      "bad it -1.83 \t bad it is -1.93\n",
      "not recommend -1.82 \t would not recommend -2.47\n",
      "not recommend -1.82 \t not recommend this -2.04\n",
      "be funny -1.81 \t to be funny -1.90\n"
     ]
    }
   ],
   "source": [
    "for i in two_gram:\n",
    "    for j in three_gram:\n",
    "        after_weight = float(j[1])\n",
    "        previous_weight = float(i[1])\n",
    "        if i[0] in j[0] and np.absolute(after_weight)>np.absolute(previous_weight):\n",
    "            print(i[0], i[1],'\\t', j[0], j[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the worst movie -3.66 \t of the worst movies -4.36\n",
      "the worst movie -3.66 \t the worst movie i -3.94\n",
      "not waste your -3.53 \t do not waste your -3.63\n",
      "do not waste -3.30 \t do not waste your -3.63\n",
      "waste your time -3.28 \t not waste your time -3.32\n",
      "of the worst -3.24 \t of the worst movies -4.36\n",
      "of the worst -3.24 \t one of the worst -3.42\n",
      "a waste of -2.81 \t a waste of time -2.97\n",
      "is a must 2.30 \t is a must see 2.41\n",
      "a must see 2.28 \t a must see for 2.67\n",
      "a must see 2.28 \t is a must see 2.41\n",
      "is not even -2.14 \t it is not even -2.70\n",
      "bad it is -1.93 \t so bad it is -2.22\n",
      "not watch this -1.87 \t do not watch this -2.20\n",
      "of the best 1.82 \t one of the best 1.98\n",
      "are supposed to -1.57 \t are supposed to be -1.68\n",
      "is a great 1.45 \t this is a great 2.54\n",
      "is a great 1.45 \t it is a great 1.88\n",
      "i first saw 1.39 \t i first saw this 1.76\n",
      "is supposed to -1.38 \t is supposed to be -1.53\n",
      "supposed to be -1.37 \t supposed to be a -1.82\n",
      "supposed to be -1.37 \t are supposed to be -1.68\n",
      "supposed to be -1.37 \t is supposed to be -1.53\n",
      "one of my 1.31 \t is one of my 1.61\n",
      "one of my 1.31 \t one of my favorite 1.43\n",
      "of the greatest 1.29 \t one of the greatest 1.42\n"
     ]
    }
   ],
   "source": [
    "for i in three_gram:\n",
    "    for j in four_gram:\n",
    "        after_weight = float(j[1])\n",
    "        previous_weight = float(i[1])\n",
    "        if i[0] in j[0] and np.absolute(after_weight)>np.absolute(previous_weight):\n",
    "            print(i[0], i[1],'\\t', j[0], j[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of the worst movies -4.36 \t one of the worst movies -4.32\n",
      "worst movie i have -3.96 \t the worst movie i have -3.85\n",
      "worst movie i have -3.96 \t worst movie i have ever -3.73\n",
      "the worst movie i -3.94 \t the worst movie i have -3.85\n",
      "do not waste your -3.63 \t do not waste your time -3.42\n",
      "one of the worst -3.42 \t one of the worst movies -4.32\n",
      "one of the worst -3.42 \t is one of the worst -3.73\n",
      "not waste your time -3.32 \t do not waste your time -3.42\n",
      "one of the best 1.98 \t is one of the best 2.35\n",
      "supposed to be a -1.82 \t is supposed to be a -1.95\n",
      "is supposed to be -1.53 \t is supposed to be a -1.95\n",
      "movie i have ever -1.45 \t worst movie i have ever -3.73\n",
      "movie i have ever -1.45 \t movie i have ever seen -1.41\n",
      "do not get me -1.00 \t do not get me wrong -0.90\n",
      "rest of the movie -0.97 \t the rest of the movie -1.00\n",
      "you want to see -0.96 \t if you want to see -1.15\n",
      "not get me wrong -0.90 \t do not get me wrong -0.90\n",
      "if you want to -0.87 \t if you want to see -1.15\n",
      "have ever seen i -0.86 \t i have ever seen i -0.82\n",
      "have ever seen i -0.86 \t i have ever seen in -0.61\n",
      "movies i have ever -0.86 \t movies i have ever seen -0.84\n",
      "it is one of 0.86 \t it is one of the 0.85\n",
      "have ever seen the -0.81 \t i have ever seen the -0.94\n",
      "have ever seen and -0.81 \t i have ever seen and -0.88\n",
      "you have not seen 0.75 \t if you have not seen 0.90\n",
      "if you have not 0.75 \t if you have not seen 0.90\n",
      "is one of the 0.69 \t is one of the worst -3.73\n",
      "is one of the 0.69 \t is one of the best 2.35\n",
      "is one of the 0.69 \t it is one of the 0.85\n",
      "is one of the 0.69 \t is one of the most 0.71\n",
      "is one of the 0.69 \t this is one of the 0.55\n"
     ]
    }
   ],
   "source": [
    "for i in four_gram:\n",
    "    for j in five_gram:\n",
    "        after_weight = float(j[1])\n",
    "        previous_weight = float(i[1])\n",
    "        if i[0] in j[0] and np.absolute(after_weight)>np.absolute(previous_weight):\n",
    "            print(i[0], i[1],'\\t', j[0], j[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the worst movie -3.66 \t one of the worst movies -4.32\n",
      "the worst movie -3.66 \t the worst movie i have -3.85\n",
      "do not waste -3.30 \t do not waste your time -3.42\n",
      "waste your time -3.28 \t do not waste your time -3.42\n",
      "of the worst -3.24 \t one of the worst movies -4.32\n",
      "of the worst -3.24 \t is one of the worst -3.73\n",
      "of the best 1.82 \t is one of the best 2.35\n",
      "is supposed to -1.38 \t is supposed to be a -1.95\n",
      "supposed to be -1.37 \t is supposed to be a -1.95\n",
      "ever seen and -0.78 \t i have ever seen and -0.88\n",
      "movie i have -0.77 \t the worst movie i have -3.85\n",
      "movie i have -0.77 \t worst movie i have ever -3.73\n",
      "movie i have -0.77 \t movie i have ever seen -1.41\n",
      "if you want -0.76 \t if you want to see -1.15\n",
      "ever seen the -0.72 \t i have ever seen the -0.94\n",
      "you want to -0.71 \t if you want to see -1.15\n",
      "is one of 0.67 \t is one of the best 2.35\n",
      "is one of 0.67 \t it is one of the 0.85\n",
      "is one of 0.67 \t is one of the most 0.71\n",
      "it is one 0.64 \t it is one of the 0.85\n",
      "i have ever -0.61 \t worst movie i have ever -3.73\n",
      "i have ever -0.61 \t movie i have ever seen -1.41\n",
      "i have ever -0.61 \t i have ever seen the -0.94\n",
      "i have ever -0.61 \t i have ever seen and -0.88\n",
      "i have ever -0.61 \t movies i have ever seen -0.84\n",
      "i have ever -0.61 \t i have ever seen i -0.82\n",
      "do not get -0.60 \t do not get me wrong -0.90\n",
      "you have not 0.57 \t if you have not seen 0.90\n",
      "want to see -0.57 \t if you want to see -1.15\n",
      "have ever seen -0.57 \t movie i have ever seen -1.41\n",
      "have ever seen -0.57 \t i have ever seen the -0.94\n",
      "have ever seen -0.57 \t i have ever seen and -0.88\n",
      "have ever seen -0.57 \t movies i have ever seen -0.84\n",
      "have ever seen -0.57 \t i have ever seen i -0.82\n",
      "have ever seen -0.57 \t i have ever seen in -0.61\n",
      "have not seen 0.55 \t if you have not seen 0.90\n",
      "ever seen in -0.54 \t i have ever seen in -0.61\n",
      "of the most 0.54 \t is one of the most 0.71\n",
      "movies i have -0.52 \t movies i have ever seen -0.84\n",
      "this is one 0.47 \t this is one of the 0.55\n",
      "i do not -0.39 \t i do not know what -0.80\n",
      "i do not -0.39 \t i do not want to 0.16\n",
      "to be a -0.38 \t is supposed to be a -1.95\n",
      "of the movie -0.37 \t the rest of the movie -1.00\n",
      "middle of the -0.35 \t in the middle of the -0.49\n",
      "one of the 0.32 \t is one of the best 2.35\n",
      "one of the 0.32 \t it is one of the 0.85\n",
      "one of the 0.32 \t is one of the most 0.71\n",
      "one of the 0.32 \t this is one of the 0.55\n",
      "the rest of -0.30 \t the rest of the movie -1.00\n",
      "the rest of -0.30 \t the rest of the film -0.33\n",
      "the rest of -0.30 \t the rest of the cast 0.08\n",
      "the rest of -0.30 \t and the rest of the 0.05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "parent_index = 0\n",
    "child_index = 0\n",
    "parent_node = [Node(\"\") for _ in range(len(three_gram))]\n",
    "child_node = [Node(\"\") for _ in range(len(five_gram))]\n",
    "\n",
    "for i in three_gram:\n",
    "    parent_node[parent_index] = Node(i[0])\n",
    "    for j in five_gram:\n",
    "        if i[0] in j[0] and j[1]>i[1]:\n",
    "            print(i[0], i[1],'\\t', j[0], j[1])\n",
    "            child_node = Node(j[0], parent=parent_node[parent_index])\n",
    "    parent_index = parent_index + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the worst movie\n",
      "├── one of the worst movies\n",
      "└── the worst movie i have\n",
      "do not waste\n",
      "└── do not waste your time\n",
      "waste your time\n",
      "└── do not waste your time\n",
      "of the worst\n",
      "├── one of the worst movies\n",
      "└── is one of the worst\n",
      "of the best\n",
      "└── is one of the best\n",
      "is supposed to\n",
      "└── is supposed to be a\n",
      "supposed to be\n",
      "└── is supposed to be a\n",
      "ever seen and\n",
      "└── i have ever seen and\n",
      "movie i have\n",
      "├── the worst movie i have\n",
      "├── worst movie i have ever\n",
      "└── movie i have ever seen\n",
      "if you want\n",
      "└── if you want to see\n",
      "ever seen the\n",
      "└── i have ever seen the\n",
      "you want to\n",
      "└── if you want to see\n",
      "is one of\n",
      "├── is one of the best\n",
      "├── it is one of the\n",
      "└── is one of the most\n",
      "it is one\n",
      "└── it is one of the\n",
      "i have ever\n",
      "├── worst movie i have ever\n",
      "├── movie i have ever seen\n",
      "├── i have ever seen the\n",
      "├── i have ever seen and\n",
      "├── movies i have ever seen\n",
      "└── i have ever seen i\n",
      "do not get\n",
      "└── do not get me wrong\n",
      "you have not\n",
      "└── if you have not seen\n",
      "want to see\n",
      "└── if you want to see\n",
      "have ever seen\n",
      "├── movie i have ever seen\n",
      "├── i have ever seen the\n",
      "├── i have ever seen and\n",
      "├── movies i have ever seen\n",
      "├── i have ever seen i\n",
      "└── i have ever seen in\n",
      "have not seen\n",
      "└── if you have not seen\n",
      "ever seen in\n",
      "└── i have ever seen in\n",
      "of the most\n",
      "└── is one of the most\n",
      "movies i have\n",
      "└── movies i have ever seen\n",
      "this is one\n",
      "└── this is one of the\n",
      "i do not\n",
      "├── i do not know what\n",
      "└── i do not want to\n",
      "to be a\n",
      "└── is supposed to be a\n",
      "of the movie\n",
      "└── the rest of the movie\n",
      "middle of the\n",
      "└── in the middle of the\n",
      "one of the\n",
      "├── is one of the best\n",
      "├── it is one of the\n",
      "├── is one of the most\n",
      "└── this is one of the\n",
      "the rest of\n",
      "├── the rest of the movie\n",
      "├── the rest of the film\n",
      "├── the rest of the cast\n",
      "└── and the rest of the\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(three_gram)):\n",
    "    for pre, fill, node in RenderTree(parent_node[i]):\n",
    "        if parent_node[i].height != 0:\n",
    "            print(\"%s%s\" % (pre, node.name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
