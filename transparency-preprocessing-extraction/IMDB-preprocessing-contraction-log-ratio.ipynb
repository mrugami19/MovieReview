{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############ Compulsory Standard Library #################\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import graphviz\n",
    "from anytree import Node, RenderTree\n",
    "from textblob import TextBlob\n",
    "\n",
    "############ Sklearn pre-processing Library #################\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "############ Sklearn model Library #################\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_imdb(path):\n",
    "    \n",
    "    print(\"Loading the imdb data\")\n",
    "    \n",
    "    train_neg_files = glob.glob(path+\"/train/neg/*.txt\")\n",
    "    train_pos_files = glob.glob(path+\"/train/pos/*.txt\")\n",
    "    \n",
    "    X_train_corpus = []\n",
    "    y_train = []\n",
    "    \n",
    "    for tnf in train_neg_files:\n",
    "        f = open(tnf, 'r', encoding=\"utf8\")\n",
    "        line = f.read()\n",
    "        #line = line[:len(line)/2]\n",
    "        X_train_corpus.append(line)\n",
    "        y_train.append(0)\n",
    "        f.close()\n",
    "    \n",
    "    for tpf in train_pos_files:\n",
    "        f = open(tpf, 'r', encoding=\"utf8\")\n",
    "        line = f.read()\n",
    "        #line = line[:len(line)/2]\n",
    "        X_train_corpus.append(line)\n",
    "        y_train.append(1)\n",
    "        f.close()\n",
    "    \n",
    "    print(\"Train Data loaded.\")\n",
    "    \n",
    "    test_neg_files = glob.glob(path+\"/test/neg/*.txt\")\n",
    "    test_pos_files = glob.glob(path+\"/test/pos/*.txt\")\n",
    "    \n",
    "    X_test_corpus = []\n",
    "    y_test = []\n",
    "    \n",
    "    for tnf in test_neg_files:\n",
    "        f = open(tnf, 'r', encoding=\"utf8\")\n",
    "        X_test_corpus.append(f.read())\n",
    "        y_test.append(0)\n",
    "        f.close()\n",
    "    \n",
    "    for tpf in test_pos_files:\n",
    "        f = open(tpf, 'r', encoding=\"utf8\")\n",
    "        X_test_corpus.append(f.read())\n",
    "        y_test.append(1)\n",
    "        f.close()\n",
    "    \n",
    "    print(\"Test Data loaded.\")\n",
    "    \n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    return X_train_corpus, y_train, X_test_corpus , y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Read and load the contraction list (or any text files)\n",
    "'''\n",
    "def load_list(filename, split_delimiter):\n",
    "    vocabulary = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for l in f:\n",
    "            vocabulary.append(l.strip().split(split_delimiter))\n",
    "    return np.asarray(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Clean the HTML tags from the corpus\n",
    "'''\n",
    "def cleanhtml(text):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantag = re.sub(cleanr, '', text)\n",
    "    cleantext = cleantag.replace('br', '')\n",
    "    return cleantext\n",
    "\n",
    "# Reference :\n",
    "# https://stackoverflow.com/questions/9662346/python-code-to-remove-html-tags-from-a-string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Replace the contraction words into two parts (by given contraction list)\n",
    "'''\n",
    "def replace_contraction(corpus, cont_list):\n",
    "    for i in range(0, cont_list.shape[0]):\n",
    "        corpus = corpus.lower().replace(cont_list[i,0], cont_list[i,1])\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Singularize the words by its POS-tag\n",
    "'''\n",
    "def word_singularize(corpus):\n",
    "    text = TextBlob(corpus)\n",
    "    for tag in text.tags:\n",
    "        if tag[1] == 'NNS' and tag[0] != 'yes':\n",
    "            corpus = corpus.replace(tag[0], tag[0].singularize())\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Update clean corpus\n",
    "'''\n",
    "def update_corpus_contraction(X_corpus):\n",
    "    cont_list = load_list(\"contraction_list.txt\", ',')\n",
    "    print(cont_list.shape)\n",
    "    print('corpus update start')\n",
    "    for i in range(0,len(X_corpus)):\n",
    "        X_corpus[i] = cleanhtml(X_corpus[i])\n",
    "        X_corpus[i] = replace_contraction(X_corpus[i], cont_list)\n",
    "        X_corpus[i] = word_singularize(X_corpus[i])\n",
    "    print('corpus update end')\n",
    "    print()\n",
    "    return X_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negative_positive_counts(X, y, word_index):\n",
    "    neg_count = np.sum(X[y==0, word_index])\n",
    "    pos_count = np.sum(X[y==1, word_index])    \n",
    "    return neg_count, pos_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_ratio_positive_negative(X, y, word_index):\n",
    "    neg_count, pos_count = negative_positive_counts(X,y, word_index)\n",
    "    log_ratio = np.log(pos_count+1)-np.log(neg_count+1)\n",
    "    return log_ratio, neg_count, pos_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_top_words_with_count(X, y, words,filename, top_k=10):\n",
    "    log_ratio = []\n",
    "    neg_count = []\n",
    "    pos_count = []\n",
    "    \n",
    "    for i in range(0,len(words)):\n",
    "        log_ratio_, neg_count_, pos_count_ = log_ratio_positive_negative(X, y, i)\n",
    "        log_ratio.append(log_ratio_)\n",
    "        neg_count.append(neg_count_)\n",
    "        pos_count.append(pos_count_)\n",
    "    \n",
    "    sorted_indices_descending_abs = np.argsort(np.absolute(log_ratio))[::-1]\n",
    "    \n",
    "    filename = filename + '.txt'\n",
    "    with open(filename, mode='w', encoding='utf8') as w:\n",
    "        for i in sorted_indices_descending_abs[: top_k]:\n",
    "#             print(\"%s\\t%0.2f\" %(words[i], weights[i]))\n",
    "#             n_p=negative_positive_counts(X, y, i)\n",
    "            w.write(\"%s\\t%0.2f\\t%d\\t%d\" %(str(words[i]), log_ratio[i], neg_count[i], pos_count[i]))\n",
    "            w.write('\\n')\n",
    "        w.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the imdb data\n",
      "Train Data loaded.\n",
      "Test Data loaded.\n"
     ]
    }
   ],
   "source": [
    "X_train_corpus , y_train, X_test_corpus , y_test = load_imdb('../aclImdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Update \n",
    "\n",
    "Such as :<br>\n",
    "[isn't $\\rightarrow$ is not] <br>\n",
    "[haven't $\\rightarrow$ have not] <br>\n",
    "ain't -> are not (despite it also could be a 'is not' or 'am not')\n",
    "<br>\n",
    "\n",
    "The steps are as follows :\n",
    "1. Remove the HTML tags\n",
    "2. Remove the contractions\n",
    "3. Singularize nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 2)\n",
      "corpus update start\n",
      "corpus update end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_corpus_update = update_corpus_contraction(X_train_corpus)\n",
    "# X_test_corpus_update = update_corpus_contraction(X_test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this film is one giant pant load. paul schrader is utterly lost in his own bad screenplay. and his directing is about as comatose as it can be without his actually having been sleepwalking during the process. the worst though is woody harrelson, whom i ordinarily like when he is properly cast. he plays \"the walker\", a homosexual man in d.c. who plays social companion to the bored wife of the washington elite. he could not have been more one dimensional if he had been cut out of a magazine and bounced around in front of the camera on a popsicle stick. his \"southern accent\" is that \"off the rack\" version that decrescendos from the beginning to the end of every line he delivers, as though the heat and humidity of the south is still draining him of every ounce of energy he has. it is monotonous. but, his is not the worst accent in the movie. his \"boyfriend\", played by moritz bleibtreau, attempt to affect some kind of a mid east accent that is so clumsy he can barely deliver the bad line written for him. he is incapable of rolling his r's in spite of the fact that in real life he is german, and speaks several language - one of them being italian! that is kind of a good reason to cast someone else do not ya think? from the story, to the screenplay, to the directing, to the camera work, to the performance by the lead, this movie is bad from beginning to end. the only tolerable moment in this film came from three supporting actress: lily tomlin, lauren bacall, and kristin scott thomas. only these three managed to make it through this movie with their dignity in tact. in fact, all three are excellent, in spite of being trapped in a really bad film. ufortunately, no one could ever be good enough to redeem this endless series of flaw. if you like these three actress, watch them in something else. this movie is not worth your time.\n",
      "[('this', 'DT'), ('film', 'NN'), ('lacked', 'VBD'), ('something', 'NN'), ('i', 'NNS'), ('could', 'MD'), ('not', 'RB'), ('put', 'VB'), ('my', 'PRP$'), ('finger', 'NN'), ('on', 'IN'), ('at', 'IN'), ('first', 'JJ'), ('charisma', 'NN'), ('on', 'IN'), ('the', 'DT'), ('part', 'NN'), ('of', 'IN'), ('the', 'DT'), ('leading', 'VBG'), ('actress', 'NN'), ('this', 'DT'), ('inevitably', 'RB'), ('translated', 'VBN'), ('to', 'TO'), ('lack', 'VB'), ('of', 'IN'), ('chemistry', 'NN'), ('when', 'WRB'), ('she', 'PRP'), ('shared', 'VBD'), ('the', 'DT'), ('screen', 'NN'), ('with', 'IN'), ('her', 'PRP$'), ('leading', 'JJ'), ('man', 'NN'), ('even', 'RB'), ('the', 'DT'), ('romantic', 'JJ'), ('scene', 'NN'), ('came', 'VBD'), ('across', 'IN'), ('as', 'IN'), ('being', 'VBG'), ('merely', 'RB'), ('the', 'DT'), ('actor', 'NN'), ('at', 'IN'), ('play', 'NN'), ('it', 'PRP'), ('could', 'MD'), ('very', 'RB'), ('well', 'RB'), ('have', 'VB'), ('been', 'VBN'), ('the', 'DT'), ('director', 'NN'), ('who', 'WP'), ('miscalculated', 'VBD'), ('what', 'WP'), ('he', 'PRP'), ('needed', 'VBD'), ('from', 'IN'), ('the', 'DT'), ('actor', 'NN'), ('i', 'NN'), ('just', 'RB'), ('do', 'VBP'), ('not', 'RB'), ('know.but', 'VB'), ('could', 'MD'), ('it', 'PRP'), ('have', 'VB'), ('been', 'VBN'), ('the', 'DT'), ('screenplay', 'NN'), ('just', 'RB'), ('exactly', 'RB'), ('who', 'WP'), ('was', 'VBD'), ('the', 'DT'), ('chef', 'NN'), ('in', 'IN'), ('love', 'NN'), ('with', 'IN'), ('he', 'PRP'), ('seemed', 'VBD'), ('more', 'RBR'), ('enamored', 'JJ'), ('of', 'IN'), ('his', 'PRP$'), ('culinary', 'JJ'), ('skill', 'NN'), ('and', 'CC'), ('restaurant', 'NN'), ('and', 'CC'), ('ultimately', 'RB'), ('of', 'IN'), ('himself', 'PRP'), ('and', 'CC'), ('his', 'PRP$'), ('youthful', 'JJ'), ('exploit', 'NN'), ('than', 'IN'), ('of', 'IN'), ('anybody', 'NN'), ('or', 'CC'), ('anything', 'NN'), ('else', 'RB'), ('he', 'PRP'), ('never', 'RB'), ('convinced', 'VBD'), ('me', 'PRP'), ('he', 'PRP'), ('was', 'VBD'), ('in', 'IN'), ('love', 'NN'), ('with', 'IN'), ('the', 'DT'), ('princess.i', 'NN'), ('was', 'VBD'), ('disappointed', 'VBN'), ('in', 'IN'), ('this', 'DT'), ('movie', 'NN'), ('but', 'CC'), ('do', 'VBP'), ('not', 'RB'), ('forget', 'VB'), ('it', 'PRP'), ('was', 'VBD'), ('nominated', 'VBN'), ('for', 'IN'), ('an', 'DT'), ('oscar', 'NN'), ('so', 'RB'), ('judge', 'NN'), ('for', 'IN'), ('yourself', 'PRP')]\n",
      "\n",
      "-> this film lacked something i could not put my finger on at first: charisma on the part of the leading actress.\n",
      "-> this inevitably translated to lack of chemistry when she shared the screen with her leading man.\n",
      "-> even the romantic scene came across as being merely the actor at play.\n",
      "-> it could very well have been the director who miscalculated what he needed from the actor.\n",
      "-> i just do not know.but could it have been the screenplay?\n",
      "-> just exactly who was the chef in love with?\n",
      "-> he seemed more enamored of his culinary skill and restaurant, and ultimately of himself and his youthful exploit, than of anybody or anything else.\n",
      "-> he never convinced me he was in love with the princess.i was disappointed in this movie.\n",
      "-> but, do not forget it was nominated for an oscar, so judge for yourself.\n"
     ]
    }
   ],
   "source": [
    "text = TextBlob(X_train_corpus_update[2])\n",
    "print(X_train_corpus[10])\n",
    "print(text.tags)\n",
    "print()\n",
    "for sentences in text.sentences:\n",
    "    print('->', sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(text.tags)\n",
    "\n",
    "# for tag in text.tags:\n",
    "#     if tag[1] == 'NNS':\n",
    "#         print(tag[0], ' ', tag[0].singularize())\n",
    "#         X_train_corpus_update[2] = X_train_corpus_update[2].replace(tag[0], tag[0].singularize())\n",
    "\n",
    "# print(X_train_corpus_update[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top 100 words extraction (1-5 grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token = r\"(?u)\\b[\\w\\'/]+\\b\"\n",
    "tf_vectorizer = CountVectorizer(lowercase=True, max_df=1.0, min_df=100, binary=True, token_pattern=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_vectorizer.set_params(ngram_range=(1,1))\n",
    "X_train = tf_vectorizer.fit_transform(X_train_corpus_update)\n",
    "words = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3684"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sort_top_words_with_count(X_train, y_train, words, '1gram', top_k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_grams_list():\n",
    "    for i in range(1,6):\n",
    "        print('Processing',i,'grams')\n",
    "        tf_vectorizer.set_params(ngram_range=(i,i))\n",
    "        X_train = tf_vectorizer.fit_transform(X_train_corpus_update)\n",
    "        words = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    #     clf = LogisticRegression(penalty='l2', C=1, random_state=42)\n",
    "    #     clf.fit(X_train, y_train)\n",
    "\n",
    "        filename=str(i)+\"gram\"\n",
    "        sort_top_words_with_count(X_train, y_train, words, filename, top_k=100)\n",
    "\n",
    "    #     del clf\n",
    "        del X_train\n",
    "        del words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 grams\n",
      "Processing 2 grams\n",
      "Processing 3 grams\n",
      "Processing 4 grams\n",
      "Processing 5 grams\n"
     ]
    }
   ],
   "source": [
    "generate_grams_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate tree relation\n",
    "\n",
    "Reference : <br>\n",
    "https://stackoverflow.com/questions/2358045/how-can-i-implement-a-tree-in-python-are-there-any-built-in-data-structures-in\n",
    "http://anytree.readthedocs.io/en/latest/ <br>\n",
    "<br>\n",
    "There is not sign changed when the top_words=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_gram = load_list(\"1gram.txt\",'\\t')\n",
    "two_gram = load_list(\"2gram.txt\",'\\t')\n",
    "three_gram = load_list(\"3gram.txt\", '\\t')\n",
    "four_gram = load_list(\"4gram.txt\", '\\t')\n",
    "five_gram = load_list(\"5gram.txt\", '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waste -2.52 \t waste your -3.31\n",
      "waste -2.52 \t waste of -2.96\n",
      "waste -2.52 \t not waste -2.92\n",
      "waste -2.52 \t a waste -2.47\n",
      "waste -2.52 \t to waste -1.95\n",
      "redeeming -2.30 \t redeeming quality -2.24\n",
      "worst -2.20 \t worst film -3.71\n",
      "worst -2.20 \t worst movie -3.49\n",
      "worst -2.20 \t the worst -2.31\n",
      "worst -2.20 \t worst of -1.92\n",
      "awful -2.13 \t awful the -3.08\n",
      "awful -2.13 \t is awful -2.95\n",
      "awful -2.13 \t was awful -2.84\n",
      "awful -2.13 \t awful i -2.78\n",
      "awful -2.13 \t awful and -2.54\n",
      "poorly -2.10 \t poorly written -2.53\n",
      "captures 2.01 \t captures the 2.51\n",
      "existent -2.00 \t non existent -1.96\n",
      "crap -1.74 \t this crap -3.86\n",
      "crap -1.74 \t of crap -2.38\n",
      "horrible -1.71 \t is horrible -2.15\n",
      "mess -1.68 \t this mess -3.38\n",
      "superb 1.68 \t is superb 1.95\n",
      "superb 1.68 \t a superb 1.83\n",
      "terrible -1.64 \t was terrible -2.65\n",
      "terrible -1.64 \t terrible the -2.48\n",
      "terrible -1.64 \t is terrible -2.41\n",
      "insult -1.62 \t an insult -2.27\n",
      "insult -1.62 \t insult to -2.21\n",
      "worse -1.61 \t is worse -2.83\n",
      "worse -1.61 \t even worse -2.39\n",
      "worse -1.61 \t worse than -2.09\n",
      "stupid -1.55 \t stupid and -1.81\n",
      "pile -1.54 \t pile of -1.95\n",
      "pleasantly 1.48 \t pleasantly surprised 1.88\n",
      "avoid -1.46 \t avoid this -2.96\n",
      "ridiculous -1.45 \t ridiculous and -1.84\n",
      "wonderful 1.44 \t is wonderful 2.19\n",
      "\n",
      "negated\n",
      "['a' 'non' 'of' 'quality' 'to']\n",
      "amplifier\n",
      "['a' 'an' 'and' 'even' 'film' 'i' 'is' 'movie' 'not' 'of' 'surprised'\n",
      " 'than' 'the' 'this' 'to' 'was' 'written' 'your']\n"
     ]
    }
   ],
   "source": [
    "negated = []\n",
    "amplifier = []\n",
    "ind_words = 0\n",
    "for i in one_gram:\n",
    "    for j in two_gram:\n",
    "        split_words = j[0].split()\n",
    "        after_weight = float(j[1])\n",
    "        previous_weight = float(i[1])\n",
    "        for k in range(0, len(split_words)):\n",
    "            if i[0] == split_words[k]:\n",
    "                if k == 0:\n",
    "                    ind_words = 1\n",
    "                elif k == 1:\n",
    "                    ind_words = 0\n",
    "                   \n",
    "                if np.sign(after_weight)!=np.sign(previous_weight) or np.absolute(after_weight) < np.absolute(previous_weight):\n",
    "                    negated.append(split_words[ind_words])\n",
    "                else:\n",
    "                    amplifier.append(split_words[ind_words])\n",
    "                print(i[0], i[1],'\\t', j[0], j[1])\n",
    "print()\n",
    "print('negated')\n",
    "print(np.unique(negated))\n",
    "print('amplifier')\n",
    "print(np.unique(amplifier))\n",
    "\n",
    "#############################################\n",
    "# top_words = 1000\n",
    "# best 0.60 \t at best -1.80\n",
    "\n",
    "# negated\n",
    "# ['at']\n",
    "# amplifier\n",
    "# []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waste -2.52 \t not waste your -3.42\n",
      "waste -2.52 \t do not waste -3.22\n",
      "waste -2.52 \t waste your time -3.19\n",
      "waste -2.52 \t waste of time -2.97\n",
      "waste -2.52 \t a waste of -2.75\n",
      "worst -2.20 \t worst movie i -4.26\n",
      "worst -2.20 \t worst film i -4.24\n",
      "worst -2.20 \t the worst film -3.85\n",
      "worst -2.20 \t the worst movie -3.77\n",
      "worst -2.20 \t of the worst -3.20\n",
      "worst -2.20 \t worst movie ever -2.92\n",
      "worst -2.20 \t is the worst -2.64\n",
      "crap -1.74 \t piece of crap -3.50\n",
      "wonderful 1.44 \t is a wonderful 1.54\n",
      "\n",
      "negated\n",
      "[]\n",
      "amplifier\n",
      "['a' 'do' 'film' 'movie' 'not' 'of' 'the' 'your']\n"
     ]
    }
   ],
   "source": [
    "negated = []\n",
    "amplifier = []\n",
    "ind_words = 0\n",
    "for i in one_gram:\n",
    "    for j in three_gram:\n",
    "        split_words = j[0].split()\n",
    "        after_weight = float(j[1])\n",
    "        previous_weight = float(i[1])\n",
    "        for k in range(0, len(split_words)):\n",
    "            if i[0] == split_words[k] and (np.absolute(after_weight)>np.absolute(previous_weight) or np.sign(after_weight)!=np.sign(previous_weight)):\n",
    "                if k == 0:\n",
    "                    ind_words = 1\n",
    "                elif k == 1:\n",
    "                    ind_words = 0\n",
    "                   \n",
    "                if np.sign(after_weight)!=np.sign(previous_weight):\n",
    "                    negated.append(split_words[ind_words])\n",
    "                else:\n",
    "                    amplifier.append(split_words[ind_words])\n",
    "                print(i[0], i[1],'\\t', j[0], j[1])\n",
    "print()\n",
    "print('negated')\n",
    "print(np.unique(negated))\n",
    "print('amplifier')\n",
    "print(np.unique(amplifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst film -3.71 \t worst film i -4.24\n",
      "worst film -3.71 \t the worst film -3.85\n",
      "worst movie -3.49 \t worst movie i -4.26\n",
      "worst movie -3.49 \t the worst movie -3.77\n",
      "worst movie -3.49 \t worst movie ever -2.92\n",
      "waste your -3.31 \t not waste your -3.42\n",
      "waste your -3.31 \t waste your time -3.19\n",
      "waste of -2.96 \t waste of time -2.97\n",
      "waste of -2.96 \t a waste of -2.75\n",
      "not waste -2.92 \t not waste your -3.42\n",
      "not waste -2.92 \t do not waste -3.22\n",
      "not funny -2.54 \t is not funny -2.46\n",
      "all cost -2.50 \t at all cost -2.58\n",
      "a waste -2.47 \t a waste of -2.75\n",
      "only good -2.40 \t the only good -2.79\n",
      "of crap -2.38 \t piece of crap -3.50\n",
      "loved this 2.38 \t i loved this 2.86\n",
      "the worst -2.31 \t the worst film -3.85\n",
      "the worst -2.31 \t the worst movie -3.77\n",
      "the worst -2.31 \t of the worst -3.20\n",
      "the worst -2.31 \t is the worst -2.64\n",
      "the worst -2.31 \t not the worst -2.04\n",
      "the worst -2.31 \t the worst of -1.81\n",
      "not worth -2.30 \t is not worth -2.52\n",
      "this piece -2.30 \t this piece of -2.76\n",
      "a must 2.17 \t is a must 2.25\n",
      "a must 2.17 \t a must see 2.25\n",
      "so bad -2.12 \t was so bad -2.84\n",
      "so bad -2.12 \t is so bad -2.65\n",
      "so bad -2.12 \t so bad that -2.30\n",
      "so bad -2.12 \t so bad it -2.20\n",
      "bad movie -2.08 \t a bad movie -1.46\n",
      "highly recommend 2.00 \t highly recommend this 2.67\n",
      "highly recommend 2.00 \t i highly recommend 2.04\n",
      "your time -1.97 \t waste your time -3.19\n",
      "loved it 1.93 \t i loved it 2.31\n",
      "worst of -1.92 \t the worst of -1.81\n",
      "great job 1.88 \t a great job 1.80\n",
      "bad i -1.88 \t so bad it -2.20\n",
      "bad i -1.88 \t bad it is -1.90\n",
      "must see 1.83 \t must see for 2.67\n",
      "must see 1.83 \t a must see 2.25\n",
      "excuse for -1.83 \t excuse for a -3.50\n",
      "bad it -1.82 \t so bad it -2.20\n",
      "bad it -1.82 \t bad it is -1.90\n",
      "not recommend -1.80 \t would not recommend -2.34\n",
      "not recommend -1.80 \t not recommend this -2.00\n",
      "be funny -1.79 \t to be funny -1.86\n",
      "no sense -1.78 \t makes no sense -1.55\n",
      "sit through -1.75 \t to sit through -1.91\n"
     ]
    }
   ],
   "source": [
    "for i in two_gram:\n",
    "    for j in three_gram:\n",
    "        after_weight = float(j[1])\n",
    "        previous_weight = float(i[1])\n",
    "        if i[0] in j[0]:\n",
    "            print(i[0], i[1],'\\t', j[0], j[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst film -3.71 \t worst film i have -4.89\n",
      "worst film -3.71 \t the worst film i -4.16\n",
      "worst movie -3.49 \t worst movie i have -4.18\n",
      "worst movie -3.49 \t the worst movie i -4.15\n",
      "worst movie -3.49 \t of the worst movie -3.98\n",
      "worst movie -3.49 \t the worst movie ever -3.03\n",
      "waste your -3.31 \t do not waste your -3.50\n",
      "waste your -3.31 \t not waste your time -3.21\n",
      "waste of -2.96 \t a waste of time -2.84\n",
      "not waste -2.92 \t do not waste your -3.50\n",
      "not waste -2.92 \t not waste your time -3.21\n",
      "a waste -2.47 \t a waste of time -2.84\n",
      "the worst -2.31 \t the worst film i -4.16\n",
      "the worst -2.31 \t the worst movie i -4.15\n",
      "the worst -2.31 \t of the worst movie -3.98\n",
      "the worst -2.31 \t one of the worst -3.36\n",
      "the worst -2.31 \t the worst movie ever -3.03\n",
      "a must 2.17 \t a must see for 2.55\n",
      "a must 2.17 \t is a must see 2.34\n",
      "so bad -2.12 \t so bad it is -2.14\n",
      "your time -1.97 \t not waste your time -3.21\n",
      "bad i -1.88 \t so bad it is -2.14\n",
      "must see 1.83 \t a must see for 2.55\n",
      "must see 1.83 \t is a must see 2.34\n",
      "bad it -1.82 \t so bad it is -2.14\n"
     ]
    }
   ],
   "source": [
    "for i in two_gram:\n",
    "    for j in four_gram:\n",
    "        after_weight = float(j[1])\n",
    "        previous_weight = float(i[1])\n",
    "        if i[0] in j[0]:\n",
    "            print(i[0], i[1],'\\t', j[0], j[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in one_gram:\n",
    "    for j in five_gram:\n",
    "        after_weight = float(j[1])\n",
    "        previous_weight = float(i[1])\n",
    "        if i[0] in j[0] and np.sign(after_weight)!=np.sign(previous_weight):\n",
    "            print(i[0], i[1],'\\t', j[0], j[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst film i -4.24 \t the worst film i have -4.80\n",
      "the worst film -3.85 \t the worst film i have -4.80\n",
      "the worst movie -3.77 \t the worst movie i have -4.08\n",
      "the worst movie -3.77 \t one of the worst movie -3.93\n",
      "do not waste -3.22 \t do not waste your time -3.29\n",
      "of the worst -3.20 \t one of the worst movie -3.93\n",
      "of the worst -3.20 \t is one of the worst -3.45\n",
      "waste your time -3.19 \t do not waste your time -3.29\n",
      "of the best 1.81 \t is one of the best 2.31\n",
      "is supposed to -1.37 \t is supposed to be a -1.89\n",
      "supposed to be -1.37 \t is supposed to be a -1.89\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "parent_index = 0\n",
    "child_index = 0\n",
    "parent_node = [Node(\"\") for _ in range(len(three_gram))]\n",
    "child_node = [Node(\"\") for _ in range(len(five_gram))]\n",
    "\n",
    "for i in three_gram:\n",
    "    parent_node[parent_index] = Node(i[0])\n",
    "    for j in five_gram:\n",
    "        if i[0] in j[0] and j[1]>i[1]:\n",
    "            print(i[0], i[1],'\\t', j[0], j[1])\n",
    "            child_node = Node(j[0], parent=parent_node[parent_index])\n",
    "    parent_index = parent_index + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst film i\n",
      "└── the worst film i have\n",
      "the worst film\n",
      "└── the worst film i have\n",
      "the worst movie\n",
      "├── the worst movie i have\n",
      "└── one of the worst movie\n",
      "do not waste\n",
      "└── do not waste your time\n",
      "of the worst\n",
      "├── one of the worst movie\n",
      "└── is one of the worst\n",
      "waste your time\n",
      "└── do not waste your time\n",
      "of the best\n",
      "└── is one of the best\n",
      "is supposed to\n",
      "└── is supposed to be a\n",
      "supposed to be\n",
      "└── is supposed to be a\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(three_gram)):\n",
    "    for pre, fill, node in RenderTree(parent_node[i]):\n",
    "        if parent_node[i].height != 0:\n",
    "            print(\"%s%s\" % (pre, node.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waste -2.52 \t waste your -3.31 \t not waste your -3.42 \t do not waste your -3.50\n",
      "waste -2.52 \t waste your -3.31 \t not waste your -3.42 \t not waste your time -3.21\n",
      "waste -2.52 \t waste your -3.31 \t waste your time -3.19 \t not waste your time -3.21\n",
      "waste -2.52 \t waste of -2.96 \t waste of time -2.97 \t a waste of time -2.84\n",
      "waste -2.52 \t waste of -2.96 \t a waste of -2.75 \t a waste of time -2.84\n",
      "waste -2.52 \t not waste -2.92 \t not waste your -3.42 \t do not waste your -3.50\n",
      "waste -2.52 \t not waste -2.92 \t not waste your -3.42 \t not waste your time -3.21\n",
      "waste -2.52 \t not waste -2.92 \t do not waste -3.22 \t do not waste your -3.50\n",
      "waste -2.52 \t a waste -2.47 \t a waste of -2.75 \t a waste of time -2.84\n",
      "worst -2.20 \t worst film -3.71 \t worst film i -4.24 \t worst film i have -4.89\n",
      "worst -2.20 \t worst film -3.71 \t worst film i -4.24 \t the worst film i -4.16\n",
      "worst -2.20 \t worst film -3.71 \t the worst film -3.85 \t the worst film i -4.16\n",
      "worst -2.20 \t worst movie -3.49 \t worst movie i -4.26 \t worst movie i have -4.18\n",
      "worst -2.20 \t worst movie -3.49 \t worst movie i -4.26 \t the worst movie i -4.15\n",
      "worst -2.20 \t worst movie -3.49 \t the worst movie -3.77 \t the worst movie i -4.15\n",
      "worst -2.20 \t worst movie -3.49 \t the worst movie -3.77 \t of the worst movie -3.98\n",
      "worst -2.20 \t worst movie -3.49 \t the worst movie -3.77 \t the worst movie ever -3.03\n",
      "worst -2.20 \t worst movie -3.49 \t worst movie ever -2.92 \t the worst movie ever -3.03\n",
      "worst -2.20 \t the worst -2.31 \t the worst film -3.85 \t the worst film i -4.16\n",
      "worst -2.20 \t the worst -2.31 \t the worst movie -3.77 \t the worst movie i -4.15\n",
      "worst -2.20 \t the worst -2.31 \t the worst movie -3.77 \t of the worst movie -3.98\n",
      "worst -2.20 \t the worst -2.31 \t the worst movie -3.77 \t the worst movie ever -3.03\n",
      "worst -2.20 \t the worst -2.31 \t of the worst -3.20 \t of the worst movie -3.98\n",
      "worst -2.20 \t the worst -2.31 \t of the worst -3.20 \t one of the worst -3.36\n"
     ]
    }
   ],
   "source": [
    "parent_index = 0\n",
    "two_index = 0\n",
    "three_index = 0\n",
    "four_index = 0\n",
    "parent_node = [Node(\"\") for _ in range(len(one_gram))]\n",
    "two_node = [Node(\"\") for _ in range(len(two_gram))]\n",
    "three_node = [Node(\"\") for _ in range(len(three_gram))]\n",
    "four_node = [Node(\"\") for _ in range(len(four_gram))]\n",
    "\n",
    "for one in one_gram:\n",
    "    parent_node[parent_index] = Node(one[0])\n",
    "    for two in two_gram:\n",
    "        if one[0] in two[0]:\n",
    "            two_node[two_index] = Node(two[0], parent=parent_node[parent_index])\n",
    "            for three in three_gram:\n",
    "                if two[0] in three[0]:\n",
    "                    three_node[three_index] = Node(three[0], parent=two_node[two_index])\n",
    "                    for four in four_gram:\n",
    "                        if three[0] in four[0]:\n",
    "                            four_node[four_index] = Node(four[0], parent=three_node[three_index])\n",
    "                            four_index = four_index + 1\n",
    "                            print(one[0], one[1],'\\t', two[0], two[1], '\\t', three[0], three[1], '\\t', four[0], four[1])\n",
    "                    three_index = three_index + 1\n",
    "            two_index = two_index + 1\n",
    "    parent_index = parent_index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waste\n",
      "├── waste your\n",
      "│   ├── not waste your\n",
      "│   │   ├── do not waste your\n",
      "│   │   └── not waste your time\n",
      "│   └── waste your time\n",
      "│       └── not waste your time\n",
      "├── waste of\n",
      "│   ├── waste of time\n",
      "│   │   └── a waste of time\n",
      "│   └── a waste of\n",
      "│       └── a waste of time\n",
      "├── not waste\n",
      "│   ├── not waste your\n",
      "│   │   ├── do not waste your\n",
      "│   │   └── not waste your time\n",
      "│   └── do not waste\n",
      "│       └── do not waste your\n",
      "├── a waste\n",
      "│   └── a waste of\n",
      "│       └── a waste of time\n",
      "└── to waste\n",
      "redeeming\n",
      "└── redeeming quality\n",
      "worst\n",
      "├── worst film\n",
      "│   ├── worst film i\n",
      "│   │   ├── worst film i have\n",
      "│   │   └── the worst film i\n",
      "│   └── the worst film\n",
      "│       └── the worst film i\n",
      "├── worst movie\n",
      "│   ├── worst movie i\n",
      "│   │   ├── worst movie i have\n",
      "│   │   └── the worst movie i\n",
      "│   ├── the worst movie\n",
      "│   │   ├── the worst movie i\n",
      "│   │   ├── of the worst movie\n",
      "│   │   └── the worst movie ever\n",
      "│   └── worst movie ever\n",
      "│       └── the worst movie ever\n",
      "├── the worst\n",
      "│   ├── the worst film\n",
      "│   │   └── the worst film i\n",
      "│   ├── the worst movie\n",
      "│   │   ├── the worst movie i\n",
      "│   │   ├── of the worst movie\n",
      "│   │   └── the worst movie ever\n",
      "│   ├── of the worst\n",
      "│   │   ├── of the worst movie\n",
      "│   │   └── one of the worst\n",
      "│   ├── is the worst\n",
      "│   ├── not the worst\n",
      "│   └── the worst of\n",
      "└── worst of\n",
      "    └── the worst of\n",
      "awful\n",
      "├── awful the\n",
      "├── is awful\n",
      "├── was awful\n",
      "├── awful i\n",
      "└── awful and\n",
      "poorly\n",
      "└── poorly written\n",
      "captures\n",
      "└── captures the\n",
      "existent\n",
      "└── non existent\n",
      "crap\n",
      "├── this crap\n",
      "└── of crap\n",
      "    └── piece of crap\n",
      "horrible\n",
      "└── is horrible\n",
      "mess\n",
      "└── this mess\n",
      "superb\n",
      "├── is superb\n",
      "└── a superb\n",
      "terrible\n",
      "├── was terrible\n",
      "├── terrible the\n",
      "└── is terrible\n",
      "insult\n",
      "├── an insult\n",
      "└── insult to\n",
      "worse\n",
      "├── is worse\n",
      "├── even worse\n",
      "└── worse than\n",
      "stupid\n",
      "└── stupid and\n",
      "pile\n",
      "└── pile of\n",
      "pleasantly\n",
      "└── pleasantly surprised\n",
      "avoid\n",
      "└── avoid this\n",
      "ridiculous\n",
      "└── ridiculous and\n",
      "wonderful\n",
      "└── is wonderful\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(one_gram)):\n",
    "    for pre, fill, node in RenderTree(parent_node[i]):\n",
    "        if parent_node[i].height != 0:\n",
    "            print(\"%s%s\" % (pre, node.name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
