{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "def load_list(filename):\n",
    "    vocabulary = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for l in f:\n",
    "            vocabulary.append(l.strip())\n",
    "    return np.asarray(vocabulary)\n",
    "\n",
    "def load_csv(filename):\n",
    "    import csv\n",
    "    \n",
    "    sentence = []\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            sentence.append(str(row))\n",
    "    return np.asarray(sentence).flatten()\n",
    "    \n",
    "pos_related = load_csv('./sentence_data/pos_related.csv')\n",
    "neg_related = load_csv('./sentence_data/neg_related.csv')\n",
    "pos_unrelated = load_csv('./sentence_data/pos_unrelated.csv')\n",
    "neg_unrelated = load_csv('./sentence_data/neg_unrelated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500,)\n",
      "(500,)\n",
      "(500,)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "print(pos_related.shape)\n",
    "print(neg_related.shape)\n",
    "print(pos_unrelated.shape)\n",
    "print(neg_unrelated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"['not perfect by a long shot, but definitely good for a smile on a bad day.']\"\n",
      " \"['the whole cast was great, each character had their own personality and charm.']\"\n",
      " '[\\'even though it has one of the standard \"revenge price plot,\" this film is my favorite of vincent price\\\\\\'s work.\\']'\n",
      " \"['i really enjoyed this movie, it is really fun to watch get elvira into all these adventure, she is just great.']\"\n",
      " \"['with more laugh than any other third-in-a-disney-series movie, hakuna matata is worth watching - if only for the hot tub scene which is still funny despite being a little bit predictable.']\"\n",
      " \"['it is really a wonderful thriller i enjoyed very much']\"\n",
      " \"['when my sister said this movie was gonna be good i had second thought but i watched it and it was actually funny']\"\n",
      " \"['it touched me in a way that, even all these year later, still affects me.']\"\n",
      " \"['i strongly recommend seeing for all']\"\n",
      " \"['without a doubt, the best late night television ever.']\"]\n"
     ]
    }
   ],
   "source": [
    "print(pos_related[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"['i had numerous problem with this film']\"\n",
      " \"['dear god i do not know where to start why this movie sucked too much']\"\n",
      " \"['i was pretty disappointed']\"\n",
      " \"['if you are tempted to watch this movie, rip your eyeball out and flush them down the toilet']\"\n",
      " \"['the music there was was annoying, and boring']\"\n",
      " \"['someone must have been seriously joking when they made this film']\"\n",
      " \"['ugly then, uglier now']\" \"['this film is predictable']\"\n",
      " '[\\'even the supporting male character are all \"bad\"\\']'\n",
      " \"['trust me, this is one let down movie that you want to avoid and this comes from one huge denzel washington fan']\"]\n"
     ]
    }
   ],
   "source": [
    "print(neg_related[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "related_set = np.hstack((pos_related, neg_related))\n",
    "print(related_set.shape)\n",
    "y_related = np.ones(related_set.shape)\n",
    "print(y_related.shape)\n",
    "\n",
    "# pos_set = pos_related\n",
    "# y_pos = np.ones(pos_set.shape)\n",
    "# neg_set = neg_related\n",
    "# y_neg = np.zeros(neg_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "unrelated_set = np.hstack((pos_unrelated, neg_unrelated))\n",
    "print(unrelated_set.shape)\n",
    "y_unrelated = np.zeros(unrelated_set.shape)\n",
    "print(y_unrelated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.hstack((related_set, unrelated_set))\n",
    "y = np.hstack((y_related, y_unrelated))\n",
    "\n",
    "# X = np.hstack((pos_set, neg_set))\n",
    "# y = np.hstack((y_pos, y_neg))\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "token = r\"(?u)\\b[\\w\\'/]+\\b\"\n",
    "tf_vectorizer = CountVectorizer(lowercase=True, max_df=1.0, min_df=1, binary=True, token_pattern=token)\n",
    "tf_vectorizer.set_params(ngram_range=(1,1))\n",
    "\n",
    "X_vectorized = tf_vectorizer.fit_transform(X)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5770\n",
      "['badly', 'badmitton', 'baffled', 'bag', 'baked', 'baker', 'bakery', 'bakshi', 'bakula', 'balance', 'balanced', 'ball', \"ballentine's\", 'ballet', 'bam', 'band', 'bandage', 'bang', 'banker', 'bankroll', 'banned', 'bar', 'barbara', 'bare', 'barely', 'bargain', 'barrel', 'barry', 'bart', 'based', 'basement', 'basic', 'basically', 'basis', 'basket', 'bat', 'batch', 'bathing', 'bathroom', 'battleship', 'bauer', 'bava', 'bb', 'bbc', 'bbc2', 'be', 'beach', 'bean', 'bear', 'beast', 'beat', \"beatty's\", 'beautiful', 'beautifully', 'became', 'because', 'beck', 'beckettian', 'become', 'becomes', 'becoming', 'bedknobs', 'bedroom', 'been', 'beer', 'before', 'began', 'begin', 'beginning', 'begins', 'behaves', 'behavior', 'behind', 'being', 'bela', 'belching', 'belief', 'believable', 'believe', 'believed', 'believer', 'bell', 'bellocchio', 'belongs', 'beloved', 'below', 'ben', 'benet', 'bent', 'berenger', 'berkley', 'beryl', 'beside', 'besides', 'best', 'bet', 'bette', 'better', 'bettered', 'between']\n"
     ]
    }
   ],
   "source": [
    "words = tf_vectorizer.get_feature_names()\n",
    "print(len(words))\n",
    "print(words[500:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1340,)\n",
      "(660,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :  0.98731\n",
      "Test :  0.76818\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_predict = clf.predict(X_test)\n",
    "\n",
    "print('Train : ', np.around(clf.score(X_train, y_train),5))\n",
    "print('Test : ', np.around(clf.score(X_test, y_test),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "665\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(y_train==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5856697819314641"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "188./(188+133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[246  79]\n",
      " [ 74 261]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, recall_score, classification_report\n",
    "\n",
    "print(confusion_matrix(y_test, y_predict, labels=[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.768"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Balanced Accuracy\n",
    "\n",
    "np.around(recall_score(y_test, y_predict,average='weighted'), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.77      0.78      0.77       335\n",
      "        1.0       0.77      0.76      0.76       325\n",
      "\n",
      "avg / total       0.77      0.77      0.77       660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = clf.coef_.flatten()\n",
    "def negative_positive_counts(X, y, word_index):\n",
    "    neg_count = np.sum(X[y==0, word_index])\n",
    "    pos_count = np.sum(X[y==1, word_index])    \n",
    "    return neg_count, pos_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\tweight\trelated\tunrelated\n",
      "when\t-1.43\t14\t65\n",
      "remember\t-1.36\t1\t19\n",
      "he\t-1.22\t13\t64\n",
      "day\t-1.12\t5\t16\n",
      "man\t-1.11\t3\t21\n",
      "she\t-1.05\t3\t24\n",
      "entire\t-1.04\t0\t3\n",
      "saw\t-0.99\t4\t15\n",
      "art\t-0.98\t0\t7\n",
      "they\t-0.96\t14\t45\n",
      "know\t-0.90\t5\t16\n",
      "about\t-0.86\t20\t43\n",
      "young\t-0.85\t1\t14\n",
      "into\t-0.84\t6\t29\n",
      "come\t-0.82\t1\t5\n",
      "big\t-0.81\t6\t13\n",
      "doing\t-0.80\t0\t6\n",
      "from\t-0.80\t30\t59\n",
      "between\t-0.80\t4\t11\n",
      "fear\t-0.79\t0\t4\n",
      "theater\t-0.78\t1\t8\n",
      "documantary\t-0.75\t1\t2\n",
      "we\t-0.74\t8\t22\n",
      "management\t-0.74\t0\t2\n",
      "made\t-0.73\t19\t16\n",
      "imagine\t-0.72\t0\t4\n",
      "behind\t-0.71\t0\t6\n",
      "porn\t-0.70\t0\t4\n",
      "why\t-0.70\t5\t7\n",
      "around\t-0.70\t2\t9\n",
      "event\t-0.70\t1\t5\n",
      "child\t-0.69\t0\t8\n",
      "before\t-0.69\t2\t8\n",
      "year\t-0.69\t10\t26\n",
      "comment\t-0.69\t0\t4\n",
      "look\t-0.68\t3\t11\n",
      "his\t-0.68\t14\t47\n",
      "week\t-0.66\t0\t8\n",
      "her\t-0.65\t6\t28\n",
      "brother\t-0.64\t0\t7\n",
      "intelligent\t-0.64\t1\t3\n",
      "guy\t-0.64\t4\t13\n",
      "verhoeven\t-0.63\t0\t2\n",
      "after\t-0.61\t12\t20\n",
      "reading\t-0.61\t2\t7\n",
      "house\t-0.61\t2\t7\n",
      "american\t-0.61\t1\t7\n",
      "in\t-0.61\t110\t177\n",
      "thing\t-0.61\t10\t14\n",
      "world\t-0.60\t1\t9\n"
     ]
    }
   ],
   "source": [
    "not_related_indices = np.argsort(weights)\n",
    "related_indices = not_related_indices[::-1]\n",
    "\n",
    "print(\"word\\tweight\\trelated\\tunrelated\")\n",
    "for i in not_related_indices[:50]:\n",
    "    unrel_cnt, rel_cnt = negative_positive_counts(X_train, y_train, i)\n",
    "    print(\"%s\\t%0.2f\\t%d\\t%d\" %(words[i], weights[i], rel_cnt, unrel_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\t\tweight\trelated\tunrelated\n",
      "disappointed\t\t1.47\t12\t0\n",
      "disappointment\t\t1.33\t7\t0\n",
      "recommend\t\t1.29\t20\t0\n",
      "boring\t\t1.29\t13\t1\n",
      "worth\t\t1.27\t19\t0\n",
      "acting\t\t1.21\t24\t2\n",
      "must\t\t1.19\t17\t3\n",
      "awful\t\t1.18\t13\t1\n",
      "this\t\t1.17\t273\t113\n",
      "character\t\t1.14\t28\t9\n",
      "ever\t\t1.14\t33\t5\n",
      "worse\t\t1.10\t8\t0\n",
      "watch\t\t1.08\t27\t10\n",
      "best\t\t1.08\t23\t6\n",
      "loved\t\t1.03\t7\t2\n",
      "very\t\t1.02\t35\t7\n",
      "scene\t\t1.00\t18\t6\n",
      "excellent\t\t1.00\t14\t1\n",
      "pretty\t\t0.99\t17\t2\n",
      "most\t\t0.99\t28\t10\n",
      "perfect\t\t0.99\t8\t1\n",
      "funny\t\t0.97\t17\t4\n",
      "value\t\t0.96\t7\t1\n",
      "otherwise\t\t0.95\t4\t0\n",
      "all\t\t0.94\t62\t31\n",
      "good\t\t0.92\t39\t14\n",
      "movie\t\t0.92\t175\t77\n",
      "recommended\t\t0.92\t6\t0\n",
      "film\t\t0.91\t134\t61\n",
      "nothing\t\t0.90\t15\t4\n",
      "performance\t\t0.89\t11\t3\n",
      "line\t\t0.88\t9\t3\n",
      "bad\t\t0.87\t23\t8\n",
      "silly\t\t0.87\t7\t1\n",
      "interesting\t\t0.86\t10\t1\n",
      "script\t\t0.85\t13\t1\n",
      "extremely\t\t0.84\t7\t0\n",
      "trash\t\t0.82\t4\t0\n",
      "horrible\t\t0.82\t10\t1\n",
      "still\t\t0.82\t15\t7\n",
      "waste\t\t0.80\t10\t1\n",
      "really\t\t0.79\t31\t17\n",
      "piece\t\t0.78\t8\t2\n",
      "done\t\t0.76\t7\t1\n",
      "enjoy\t\t0.76\t7\t0\n",
      "even\t\t0.75\t24\t16\n",
      "worst\t\t0.75\t18\t1\n",
      "great\t\t0.75\t21\t9\n",
      "rather\t\t0.74\t10\t3\n",
      "no\t\t0.74\t24\t14\n"
     ]
    }
   ],
   "source": [
    "print(\"word\\t\\tweight\\trelated\\tunrelated\")\n",
    "for i in related_indices[:50]:\n",
    "    unrel_cnt, rel_cnt = negative_positive_counts(X_train, y_train, i)\n",
    "    print(\"%s\\t\\t%0.2f\\t%d\\t%d\" %(words[i], weights[i], rel_cnt, unrel_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\tweight\trelated\tunrelated\n",
      "disappointed\t1.47\t12\t0\n",
      "when\t-1.43\t14\t65\n",
      "remember\t-1.36\t1\t19\n",
      "disappointment\t1.33\t7\t0\n",
      "recommend\t1.29\t20\t0\n",
      "boring\t1.29\t13\t1\n",
      "worth\t1.27\t19\t0\n",
      "he\t-1.22\t13\t64\n",
      "acting\t1.21\t24\t2\n",
      "must\t1.19\t17\t3\n",
      "awful\t1.18\t13\t1\n",
      "this\t1.17\t273\t113\n",
      "character\t1.14\t28\t9\n",
      "ever\t1.14\t33\t5\n",
      "day\t-1.12\t5\t16\n",
      "man\t-1.11\t3\t21\n",
      "worse\t1.10\t8\t0\n",
      "watch\t1.08\t27\t10\n",
      "best\t1.08\t23\t6\n",
      "she\t-1.05\t3\t24\n"
     ]
    }
   ],
   "source": [
    "abs_indices = np.argsort(np.absolute(weights))[::-1]\n",
    "\n",
    "print(\"word\\tweight\\trelated\\tunrelated\")\n",
    "for i in abs_indices[:20]:\n",
    "    unrel_cnt, rel_cnt = negative_positive_counts(X_train, y_train, i)\n",
    "    print(\"%s\\t%0.2f\\t%d\\t%d\" %(words[i], weights[i], rel_cnt, unrel_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not perfect by a long shot, but definitely good for a smile on a bad day.']\n",
      "['the whole cast was great, each character had their own personality and charm.']\n",
      "['even though it has one of the standard \"revenge price plot,\" this film is my favorite of vincent price\\'s work.']\n",
      "['i really enjoyed this movie, it is really fun to watch get elvira into all these adventure, she is just great.']\n",
      "['with more laugh than any other third-in-a-disney-series movie, hakuna matata is worth watching - if only for the hot tub scene which is still funny despite being a little bit predictable.']\n",
      "['it is really a wonderful thriller i enjoyed very much']\n",
      "['when my sister said this movie was gonna be good i had second thought but i watched it and it was actually funny']\n",
      "['it touched me in a way that, even all these year later, still affects me.']\n",
      "['i strongly recommend seeing for all']\n",
      "['without a doubt, the best late night television ever.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in X[:10] :\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(path, X):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(X, f)\n",
    "def open_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    return X\n",
    "\n",
    "X_te_clean = open_pickle(\"./pickles/imdb_x_te_clean.pickle\")\n",
    "y_te = open_pickle(\"./pickles/imdb_y_te.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sentence(corpus):\n",
    "    from textblob import TextBlob\n",
    "    text = TextBlob(corpus)\n",
    "    i = 0\n",
    "    sent = []\n",
    "    for sentence in text.raw_sentences:\n",
    "        sent.append(sentence)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 : this was an excellent show.\n",
      "0.0 : it came on pbs back home in chicago and i remember cindy herron (from envogue) played the teen aged daughter.\n",
      "0.0 : the show dealt with subject such as sex, peer pressure and puberty.\n",
      "0.0 : it was about a middle class black family who had a teen aged daughter and son who moved to a middle class neighborhood from oakland or somewhere (i can not remember).\n",
      "0.0 : i remember several episode but the one i remember most was when their cousin got her period for the first time.\n",
      "0.0 : i was probably 7-8 when i first watched it and i was able to keep up with the program.\n",
      "1.0 : this was a great show.\n",
      "0.0 : i can not remember the name of the guy who played the son on the show, but i always got him confused with kevin hook.\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "test = print_sentence(X_te_clean[0])\n",
    "\n",
    "test_matrix = tf_vectorizer.transform(test) \n",
    "test_matrix.shape\n",
    "\n",
    "y_pred_test = clf.predict(test_matrix)\n",
    "for i in range(len(test)):\n",
    "    print(y_pred_test[i], ':' , test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "- age cannot tarnish the beauty of this east-west love story for me.\n",
      "- as mark elliott, william holden is intelligent, breezy and a bit weak; jennifer jone is perhaps well-nigh-perfect as dr. han suyin, by turns doubt-torn and ecstatic, eager and hesitant.\n",
      "- other in the large cast include torin thatcher, isobel elsom, murray matheson, virginia gregg, richard loo, soo yong, philip ahn, jorja curtright and donna martell; many of hollywood's best oriental actor played smaller uncredited part also.\n",
      "- the film is unarguably physically busy, interesting and often beautiful also.\n",
      "- with cinematography by leon shamroy, ben nye's makeup and helen turpin's hairstyle, the great work by set decorator, sound and lighting, art department and all concerned, this has to be one of the most memorable production set in a major non-u.s. city of all time, and one of the most difficult to capture on film.\n",
      "- truly, love is a many-splendored thing, dr. han says; and this movie stands as one of that doctrine's shining proofs, lucent as a pearl, timeless as a chinese proverb and lovely as polished jade set against a rough background.\n"
     ]
    }
   ],
   "source": [
    "test = print_sentence(X_te_clean[1])\n",
    "\n",
    "test_matrix = tf_vectorizer.transform(test) \n",
    "test_matrix.shape\n",
    "\n",
    "y_pred_test = clf.predict(test_matrix)\n",
    "x_extracted_1 = ''\n",
    "print(y_te[1])\n",
    "for i in range(len(test)):\n",
    "    if y_pred_test[i] == 1:\n",
    "        x_extracted_1 += test[i]\n",
    "        print('-', test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "- i have yet to watch the first entry in this series, however, fortunately, i was still able to follow the complex and intricate plot, with all its unexpected twist and turn, and i applaud them for the utter originality of the concept herein.\n",
      "- in case there is any confusion, let me leave no doubt as to the fact that everything i have just said is coated in pure, carefully nurtured sarcasm, the kind that flourish and grow exponentially when exposed to crap like this flick.\n",
      "- a clear sign that this is unimpressive is that it was directed by a visual effect creator, whose only other credit in that field is a henry rooker film that was not well received.\n",
      "- the action is not terrible.\n",
      "- cinematography and editing are fine.\n",
      "- the music is cool enough.\n",
      "- language is infrequent, if even that.\n",
      "- i recommend this solely to fan of b-movie, and i will say that you can do worse than this.\n",
      "- 1/10\n"
     ]
    }
   ],
   "source": [
    "test = print_sentence(X_te_clean[3])\n",
    "\n",
    "test_matrix = tf_vectorizer.transform(test) \n",
    "test_matrix.shape\n",
    "\n",
    "y_pred_test = clf.predict(test_matrix)\n",
    "x_extracted_3 = ''\n",
    "print(y_te[3])\n",
    "for i in range(len(test)):\n",
    "    if y_pred_test[i] == 1:\n",
    "        x_extracted_3 += test[i]\n",
    "        print('-',test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the clf 3\n",
    "\n",
    "X_tr_clean = open_pickle(\"./pickles/imdb_x_tr_clean.pickle\")\n",
    "y_tr = open_pickle(\"./pickles/imdb_y_tr.pickle\")\n",
    "cv = CountVectorizer(min_df=5, token_pattern=token)\n",
    "X_train_ = cv.fit_transform(X_tr_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_.shape\n",
    "y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_extracted = np.hstack((x_extracted_1, x_extracted_3))\n",
    "X_test_ = cv.transform(x_extracted)\n",
    "y_test_ = [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 26255)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_2 = LogisticRegression()\n",
    "clf_2.fit(X_train_, y_tr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_2.score(X_test_, y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
