{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "def load_list(filename):\n",
    "    vocabulary = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for l in f:\n",
    "            vocabulary.append(l.strip())\n",
    "    return np.asarray(vocabulary)\n",
    "\n",
    "def load_csv(filename):\n",
    "    import csv\n",
    "    \n",
    "    sentence = []\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            sentence.append(str(row))\n",
    "    return np.asarray(sentence).flatten()\n",
    "    \n",
    "pos_related = load_csv('./sentence_data/pos_related.csv')\n",
    "neg_related = load_csv('./sentence_data/neg_related.csv')\n",
    "pos_unrelated = load_csv('./sentence_data/pos_unrelated.csv')\n",
    "neg_unrelated = load_csv('./sentence_data/neg_unrelated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500,)\n",
      "(500,)\n",
      "(500,)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "print(pos_related.shape)\n",
    "print(neg_related.shape)\n",
    "print(pos_unrelated.shape)\n",
    "print(neg_unrelated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \"['not perfect by a long shot, but definitely good for a smile on a bad day.']\"\n",
      " \"['the whole cast was great, each character had their own personality and charm.']\"\n",
      " '[\\'even though it has one of the standard \"revenge price plot,\" this film is my favorite of vincent price\\\\\\'s work.\\']'\n",
      " \"['i really enjoyed this movie, it is really fun to watch get elvira into all these adventure, she is just great.']\"\n",
      " \"['with more laugh than any other third-in-a-disney-series movie, hakuna matata is worth watching - if only for the hot tub scene which is still funny despite being a little bit predictable.']\"\n",
      " \"['it is really a wonderful thriller i enjoyed very much']\"\n",
      " \"['when my sister said this movie was gonna be good i had second thought but i watched it and it was actually funny']\"\n",
      " \"['it touched me in a way that, even all these year later, still affects me.']\"\n",
      " \"['i strongly recommend seeing for all']\"\n",
      " \"['without a doubt, the best late night television ever.']\"]\n"
     ]
    }
   ],
   "source": [
    "print(pos_related[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"['i had numerous problem with this film']\"\n",
      " \"['dear god i do not know where to start why this movie sucked too much']\"\n",
      " \"['i was pretty disappointed']\"\n",
      " \"['if you are tempted to watch this movie, rip your eyeball out and flush them down the toilet']\"\n",
      " \"['the music there was was annoying, and boring']\"\n",
      " \"['someone must have been seriously joking when they made this film']\"\n",
      " \"['ugly then, uglier now']\" \"['this film is predictable']\"\n",
      " '[\\'even the supporting male character are all \"bad\"\\']'\n",
      " \"['trust me, this is one let down movie that you want to avoid and this comes from one huge denzel washington fan']\"]\n"
     ]
    }
   ],
   "source": [
    "print(neg_related[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "related_set = np.hstack((pos_related, neg_related))\n",
    "print(related_set.shape)\n",
    "y_related = np.ones(related_set.shape)\n",
    "print(y_related.shape)\n",
    "\n",
    "# pos_set = pos_related\n",
    "# y_pos = np.ones(pos_set.shape)\n",
    "# neg_set = neg_related\n",
    "# y_neg = np.zeros(neg_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "unrelated_set = np.hstack((pos_unrelated, neg_unrelated))\n",
    "print(unrelated_set.shape)\n",
    "y_unrelated = np.zeros(unrelated_set.shape)\n",
    "print(y_unrelated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.hstack((related_set, unrelated_set))\n",
    "y = np.hstack((y_related, y_unrelated))\n",
    "\n",
    "# X = np.hstack((pos_set, neg_set))\n",
    "# y = np.hstack((y_pos, y_neg))\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_pickle(path, X):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(X, f)\n",
    "def open_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    return X\n",
    "\n",
    "X_train_sent = open_pickle('./sentence_data/imdb_sentence_xtrain.pickle')\n",
    "X_test_sent = open_pickle('./sentence_data/imdb_sentence_xtest.pickle')\n",
    "y_train = open_pickle('./sentence_data/imdb_sentence_ytrain.pickle')\n",
    "y_test = open_pickle('./sentence_data/imdb_sentence_ytest.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "token = r\"(?u)\\b[\\w\\'/]+\\b\"\n",
    "tf_vectorizer = CountVectorizer(lowercase=True, max_df=1.0, min_df=1, binary=True, token_pattern=token)\n",
    "tf_vectorizer.set_params(ngram_range=(1,1))\n",
    "\n",
    "X_train = tf_vectorizer.fit_transform(X_train_sent)\n",
    "X_test = tf_vectorizer.transform(X_test_sent)\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4532\n",
      "['blond', 'blood', 'bloodbath', 'bloody', 'blossom', 'blow', 'blowa', 'blows', 'blubber', 'blvd', 'board', 'body', 'bold', 'bomb', 'bombay', 'bomber', 'bombshell', 'bonanza', 'bond', 'bondage', 'book', 'booster', 'border', 'borderlinepersonality', 'bored', 'boring', 'born', 'borrow', 'borrowed', 'bosnian', 'boss', 'bot', 'both', 'bother', 'bothered', 'boths', 'bottom', 'bought', 'bound', 'bounty', 'bourne', 'box', 'boxer', 'boxing', 'boxset', 'boy', 'boyat', 'boys', 'braggart', 'brain', 'brash', 'brashness', 'break', 'breaker', 'breakin', 'breaking', 'breaks', 'breakthis', 'breast', 'breath', 'breathable', 'breathing', 'breathless', 'breathlessly', 'brendan', 'brett', 'brian', 'bridge', 'brief', 'brilliant', 'bring', 'britian', 'british', 'brittany', 'brittonwho', 'broadcast', 'broadcasted', 'brontes', 'brooklyn', 'broomstick', 'brother', 'brought', 'brow', 'brown', 'browns', 'brutal', 'btw', 'buck', 'buckaroo', 'budget', 'building', 'built', 'bulgaria', 'bull', 'bullying', 'bumbled', 'bunch', 'burgeoning', 'burn', 'burned']\n"
     ]
    }
   ],
   "source": [
    "words = tf_vectorizer.get_feature_names()\n",
    "print(len(words))\n",
    "print(words[500:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1333,)\n",
      "(667,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['she becomes the centre of the films universe as well as our',\n",
       "       'anyone who loves the rheostatic music is going to enjoy this film',\n",
       "       'the song that were in common with the musical are better done in the movie the new one are quite good one and the whole movie just delivers more than the musical in my opinion especially compared to a musical which has few decor',\n",
       "       'disappointing and undeniably dull truecrime movie that has poorly cast character actor jeremy renner languidly mumbling his way through the title role of jeffrey dahmer who was easily one of the last centurys most recognizable degeneratesserial killer',\n",
       "       'i would not recommend to anyone that they waste the time it takes to watch it',\n",
       "       'dear god i do not know where to start why this movie sucked too much',\n",
       "       'this film really surprised me as it is a comedy masterpiece',\n",
       "       'this was a film that you can truly wet yourself laughing at',\n",
       "       'something told me to go see meet the pare nots instead',\n",
       "       'for those looking for explosion and fancy special effect you will be disappointed for those looking for a good haunted house type horror with a strong story i definitely think the shining is for you well worth a watch in my humble opinion'], \n",
       "      dtype='<U667')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sent[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :  0.98575\n",
      "Test :  0.75562\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_predict = clf.predict(X_test)\n",
    "\n",
    "print('Train : ', np.around(clf.score(X_train, y_train),5))\n",
    "print('Test : ', np.around(clf.score(X_test, y_test),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "637\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(y_train==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5856697819314641"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "188./(188+133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[240  64]\n",
      " [ 99 264]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, recall_score, classification_report\n",
    "\n",
    "print(confusion_matrix(y_test, y_predict, labels=[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75600000000000001"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Balanced Accuracy\n",
    "\n",
    "np.around(recall_score(y_test, y_predict,average='weighted'), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.80      0.73      0.76       363\n",
      "        1.0       0.71      0.79      0.75       304\n",
      "\n",
      "avg / total       0.76      0.76      0.76       667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = clf.coef_.flatten()\n",
    "def negative_positive_counts(X, y, word_index):\n",
    "    neg_count = np.sum(X[y==0, word_index])\n",
    "    pos_count = np.sum(X[y==1, word_index])    \n",
    "    return neg_count, pos_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\tweight\trelated\tunrelated\n",
      "he\t-1.42\t11\t59\n",
      "remember\t-1.35\t1\t15\n",
      "into\t-1.30\t5\t32\n",
      "we\t-1.01\t8\t24\n",
      "try\t-0.98\t2\t6\n",
      "who\t-0.98\t23\t61\n",
      "lets\t-0.95\t0\t8\n",
      "art\t-0.94\t0\t10\n",
      "about\t-0.91\t27\t48\n",
      "school\t-0.91\t0\t12\n"
     ]
    }
   ],
   "source": [
    "not_related_indices = np.argsort(weights)\n",
    "related_indices = not_related_indices[::-1]\n",
    "\n",
    "print(\"word\\tweight\\trelated\\tunrelated\")\n",
    "for i in not_related_indices[:10]:\n",
    "    unrel_cnt, rel_cnt = negative_positive_counts(X_train, y_train, i)\n",
    "    print(\"%s\\t%0.2f\\t%d\\t%d\" %(words[i], weights[i], rel_cnt, unrel_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\t\tweight\trelated\tunrelated\n",
      "awful\t\t1.45\t14\t0\n",
      "boring\t\t1.39\t11\t1\n",
      "disappointed\t\t1.35\t11\t0\n",
      "worth\t\t1.32\t16\t0\n",
      "recommend\t\t1.28\t17\t0\n",
      "performance\t\t1.25\t14\t2\n",
      "even\t\t1.24\t21\t7\n",
      "ever\t\t1.23\t31\t6\n",
      "scene\t\t1.18\t23\t8\n",
      "great\t\t1.16\t35\t9\n"
     ]
    }
   ],
   "source": [
    "print(\"word\\t\\tweight\\trelated\\tunrelated\")\n",
    "for i in related_indices[:10]:\n",
    "    unrel_cnt, rel_cnt = negative_positive_counts(X_train, y_train, i)\n",
    "    print(\"%s\\t\\t%0.2f\\t%d\\t%d\" %(words[i], weights[i], rel_cnt, unrel_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\tweight\trelated\tunrelated\n",
      "awful\t1.45\t14\t0\n",
      "he\t-1.42\t11\t59\n",
      "boring\t1.39\t11\t1\n",
      "disappointed\t1.35\t11\t0\n",
      "remember\t-1.35\t1\t15\n",
      "worth\t1.32\t16\t0\n",
      "into\t-1.30\t5\t32\n",
      "recommend\t1.28\t17\t0\n",
      "performance\t1.25\t14\t2\n",
      "even\t1.24\t21\t7\n",
      "ever\t1.23\t31\t6\n",
      "scene\t1.18\t23\t8\n",
      "great\t1.16\t35\t9\n",
      "most\t1.15\t22\t7\n",
      "must\t1.12\t13\t2\n",
      "very\t1.11\t39\t8\n",
      "movie\t1.11\t184\t67\n",
      "good\t1.05\t43\t11\n",
      "we\t-1.01\t8\t24\n",
      "this\t0.99\t281\t124\n"
     ]
    }
   ],
   "source": [
    "abs_indices = np.argsort(np.absolute(weights))[::-1]\n",
    "\n",
    "print(\"word\\tweight\\trelated\\tunrelated\")\n",
    "for i in abs_indices[:20]:\n",
    "    unrel_cnt, rel_cnt = negative_positive_counts(X_train, y_train, i)\n",
    "    print(\"%s\\t%0.2f\\t%d\\t%d\" %(words[i], weights[i], rel_cnt, unrel_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sentence in X[:10] :\n",
    "#     print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(path, X):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(X, f)\n",
    "def open_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    return X\n",
    "\n",
    "X_te_clean = open_pickle(\"./pickles/imdb_x_te_clean.pickle\")\n",
    "y_te = open_pickle(\"./pickles/imdb_y_te.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sentence(corpus):\n",
    "    from textblob import TextBlob\n",
    "    text = TextBlob(corpus)\n",
    "    i = 0\n",
    "    sent = []\n",
    "    for sentence in text.raw_sentences:\n",
    "        sent.append(sentence)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 : this was an excellent show.\n",
      "0.0 : it came on pbs back home in chicago and i remember cindy herron (from envogue) played the teen aged daughter.\n",
      "0.0 : the show dealt with subject such as sex, peer pressure and puberty.\n",
      "0.0 : it was about a middle class black family who had a teen aged daughter and son who moved to a middle class neighborhood from oakland or somewhere (i can not remember).\n",
      "0.0 : i remember several episode but the one i remember most was when their cousin got her period for the first time.\n",
      "0.0 : i was probably 7-8 when i first watched it and i was able to keep up with the program.\n",
      "1.0 : this was a great show.\n",
      "0.0 : i can not remember the name of the guy who played the son on the show, but i always got him confused with kevin hook.\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "test = print_sentence(X_te_clean[0])\n",
    "\n",
    "test_matrix = tf_vectorizer.transform(test) \n",
    "test_matrix.shape\n",
    "\n",
    "y_pred_test = clf.predict(test_matrix)\n",
    "for i in range(len(test)):\n",
    "    print(y_pred_test[i], ':' , test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "- age cannot tarnish the beauty of this east-west love story for me.\n",
      "- as mark elliott, william holden is intelligent, breezy and a bit weak; jennifer jone is perhaps well-nigh-perfect as dr. han suyin, by turns doubt-torn and ecstatic, eager and hesitant.\n",
      "- other in the large cast include torin thatcher, isobel elsom, murray matheson, virginia gregg, richard loo, soo yong, philip ahn, jorja curtright and donna martell; many of hollywood's best oriental actor played smaller uncredited part also.\n",
      "- the film is unarguably physically busy, interesting and often beautiful also.\n",
      "- with cinematography by leon shamroy, ben nye's makeup and helen turpin's hairstyle, the great work by set decorator, sound and lighting, art department and all concerned, this has to be one of the most memorable production set in a major non-u.s. city of all time, and one of the most difficult to capture on film.\n",
      "- truly, love is a many-splendored thing, dr. han says; and this movie stands as one of that doctrine's shining proofs, lucent as a pearl, timeless as a chinese proverb and lovely as polished jade set against a rough background.\n"
     ]
    }
   ],
   "source": [
    "test = print_sentence(X_te_clean[1])\n",
    "\n",
    "test_matrix = tf_vectorizer.transform(test) \n",
    "test_matrix.shape\n",
    "\n",
    "y_pred_test = clf.predict(test_matrix)\n",
    "x_extracted_1 = ''\n",
    "print(y_te[1])\n",
    "for i in range(len(test)):\n",
    "    if y_pred_test[i] == 1:\n",
    "        x_extracted_1 += test[i]\n",
    "        print('-', test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "- i have yet to watch the first entry in this series, however, fortunately, i was still able to follow the complex and intricate plot, with all its unexpected twist and turn, and i applaud them for the utter originality of the concept herein.\n",
      "- in case there is any confusion, let me leave no doubt as to the fact that everything i have just said is coated in pure, carefully nurtured sarcasm, the kind that flourish and grow exponentially when exposed to crap like this flick.\n",
      "- a clear sign that this is unimpressive is that it was directed by a visual effect creator, whose only other credit in that field is a henry rooker film that was not well received.\n",
      "- the action is not terrible.\n",
      "- cinematography and editing are fine.\n",
      "- language is infrequent, if even that.\n",
      "- violence is fairly bloody.\n",
      "- i recommend this solely to fan of b-movie, and i will say that you can do worse than this.\n"
     ]
    }
   ],
   "source": [
    "test = print_sentence(X_te_clean[3])\n",
    "\n",
    "test_matrix = tf_vectorizer.transform(test) \n",
    "test_matrix.shape\n",
    "\n",
    "y_pred_test = clf.predict(test_matrix)\n",
    "x_extracted_3 = ''\n",
    "print(y_te[3])\n",
    "for i in range(len(test)):\n",
    "    if y_pred_test[i] == 1:\n",
    "        x_extracted_3 += test[i]\n",
    "        print('-',test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the clf 3\n",
    "\n",
    "X_tr_clean = open_pickle(\"./pickles/imdb_x_tr_clean.pickle\")\n",
    "y_tr = open_pickle(\"./pickles/imdb_y_tr.pickle\")\n",
    "cv = CountVectorizer(min_df=5, token_pattern=token)\n",
    "X_train_ = cv.fit_transform(X_tr_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_.shape\n",
    "y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_extracted = np.hstack((x_extracted_1, x_extracted_3))\n",
    "X_test_ = cv.transform(x_extracted)\n",
    "y_test_ = [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 26255)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_2 = LogisticRegression()\n",
    "clf_2.fit(X_train_, y_tr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_2.score(X_test_, y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
