{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential  \n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "def load_list(filename):\n",
    "    vocabulary = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for l in f:\n",
    "            vocabulary.append(l.strip())\n",
    "    return np.asarray(vocabulary)\n",
    "\n",
    "def load_csv(filename):\n",
    "    import csv\n",
    "    \n",
    "    sentence = []\n",
    "    with open('./sentence_data/pos_related.csv', 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            sentence.append(row)\n",
    "    return np.asarray(sentence).flatten()\n",
    "    \n",
    "pos_related = load_csv('./sentence_data/pos_related.csv')\n",
    "neg_related = load_csv('./sentence_data/neg_related.csv')\n",
    "pos_unrelated = load_csv('./sentence_data/pos_unrelated.csv')\n",
    "neg_unrelated = load_csv('./sentence_data/neg_unrelated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(466,)\n",
      "(466,)\n",
      "(466,)\n",
      "(466,)\n"
     ]
    }
   ],
   "source": [
    "print(pos_related.shape)\n",
    "print(neg_related.shape)\n",
    "print(pos_unrelated.shape)\n",
    "print(neg_unrelated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(932,)\n",
      "(932,)\n",
      "(932,)\n",
      "(932,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1864,)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack the data\n",
    "\n",
    "related_set = np.hstack((pos_related, neg_related))\n",
    "print(related_set.shape)\n",
    "y_related = np.ones(related_set.shape)\n",
    "print(y_related.shape)\n",
    "\n",
    "unrelated_set = np.hstack((pos_unrelated, neg_unrelated))\n",
    "print(unrelated_set.shape)\n",
    "y_unrelated = np.zeros(unrelated_set.shape)\n",
    "print(y_unrelated.shape)\n",
    "\n",
    "X_stack = np.hstack((related_set, unrelated_set))\n",
    "y_stack = np.hstack((y_related, y_unrelated))\n",
    "\n",
    "X_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "token = r\"(?u)\\b[\\w\\'/]+\\b\"\n",
    "tf_vectorizer = CountVectorizer(lowercase=True, max_df=1.0, min_df=5, binary=False, token_pattern=token)\n",
    "tf_vectorizer.set_params(ngram_range=(1,1))\n",
    "\n",
    "# X_vectorized = tf_vectorizer.fit_transform(X_stack)\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y_stack, test_size=0.33, random_state=42)\n",
    "\n",
    "tf_vectorizer.fit(X_stack)\n",
    "\n",
    "word_dict = tf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10/10': 0,\n",
       " '15': 1,\n",
       " '2': 2,\n",
       " '3': 3,\n",
       " '5': 4,\n",
       " \"50's\": 5,\n",
       " '7': 6,\n",
       " '9': 7,\n",
       " '99': 8,\n",
       " 'a': 9,\n",
       " 'about': 10,\n",
       " 'absolute': 11,\n",
       " 'absolutely': 12,\n",
       " 'acted': 13,\n",
       " 'acting': 14,\n",
       " 'action': 15,\n",
       " 'actor': 16,\n",
       " 'actually': 17,\n",
       " 'adaptation': 18,\n",
       " 'admit': 19,\n",
       " 'after': 20,\n",
       " 'again': 21,\n",
       " 'against': 22,\n",
       " 'age': 23,\n",
       " 'all': 24,\n",
       " 'almost': 25,\n",
       " 'along': 26,\n",
       " 'already': 27,\n",
       " 'also': 28,\n",
       " 'although': 29,\n",
       " 'altogether': 30,\n",
       " 'always': 31,\n",
       " 'am': 32,\n",
       " 'amazing': 33,\n",
       " 'american': 34,\n",
       " 'among': 35,\n",
       " 'an': 36,\n",
       " 'and': 37,\n",
       " 'andrew': 38,\n",
       " 'animation': 39,\n",
       " 'ann': 40,\n",
       " 'any': 41,\n",
       " 'anyone': 42,\n",
       " 'anything': 43,\n",
       " 'anyway': 44,\n",
       " 'are': 45,\n",
       " 'around': 46,\n",
       " 'as': 47,\n",
       " 'at': 48,\n",
       " 'audience': 49,\n",
       " 'awesome': 50,\n",
       " 'back': 51,\n",
       " 'bad': 52,\n",
       " 'basically': 53,\n",
       " 'be': 54,\n",
       " 'beautiful': 55,\n",
       " 'beautifully': 56,\n",
       " 'because': 57,\n",
       " 'been': 58,\n",
       " 'before': 59,\n",
       " 'being': 60,\n",
       " 'believable': 61,\n",
       " 'best': 62,\n",
       " 'better': 63,\n",
       " 'between': 64,\n",
       " 'big': 65,\n",
       " 'bit': 66,\n",
       " 'black': 67,\n",
       " 'blaise': 68,\n",
       " 'blood': 69,\n",
       " 'bold': 70,\n",
       " 'bond': 71,\n",
       " 'book': 72,\n",
       " 'boring': 73,\n",
       " 'both': 74,\n",
       " 'brilliant': 75,\n",
       " 'british': 76,\n",
       " 'budget': 77,\n",
       " 'but': 78,\n",
       " 'by': 79,\n",
       " 'can': 80,\n",
       " 'cast': 81,\n",
       " 'casting': 82,\n",
       " 'certain': 83,\n",
       " 'certainly': 84,\n",
       " 'chance': 85,\n",
       " 'character': 86,\n",
       " 'charm': 87,\n",
       " 'choice': 88,\n",
       " 'cinema': 89,\n",
       " 'cinematography': 90,\n",
       " 'city': 91,\n",
       " 'class': 92,\n",
       " 'classic': 93,\n",
       " 'clear': 94,\n",
       " 'clever': 95,\n",
       " 'close': 96,\n",
       " 'collection': 97,\n",
       " 'comedy': 98,\n",
       " 'comic': 99,\n",
       " 'coming': 100,\n",
       " 'completely': 101,\n",
       " 'complex': 102,\n",
       " 'concept': 103,\n",
       " 'contemporary': 104,\n",
       " 'convincing': 105,\n",
       " 'could': 106,\n",
       " 'crazy': 107,\n",
       " 'create': 108,\n",
       " 'crime': 109,\n",
       " 'dark': 110,\n",
       " 'day': 111,\n",
       " 'definite': 112,\n",
       " 'definitely': 113,\n",
       " 'delight': 114,\n",
       " 'delightful': 115,\n",
       " 'describe': 116,\n",
       " 'design': 117,\n",
       " 'despite': 118,\n",
       " 'detail': 119,\n",
       " 'did': 120,\n",
       " 'different': 121,\n",
       " 'directing': 122,\n",
       " 'direction': 123,\n",
       " 'director': 124,\n",
       " 'disappointed': 125,\n",
       " 'disney': 126,\n",
       " 'do': 127,\n",
       " 'documentary': 128,\n",
       " 'does': 129,\n",
       " 'done': 130,\n",
       " 'doubt': 131,\n",
       " 'down': 132,\n",
       " 'during': 133,\n",
       " 'dvd': 134,\n",
       " 'each': 135,\n",
       " 'early': 136,\n",
       " 'edge': 137,\n",
       " 'effect': 138,\n",
       " 'else': 139,\n",
       " 'emotionally': 140,\n",
       " 'end': 141,\n",
       " 'ending': 142,\n",
       " 'enjoy': 143,\n",
       " 'enjoyed': 144,\n",
       " 'enough': 145,\n",
       " 'entertained': 146,\n",
       " 'entertaining': 147,\n",
       " 'entertainment': 148,\n",
       " 'episode': 149,\n",
       " 'equally': 150,\n",
       " 'especially': 151,\n",
       " 'essential': 152,\n",
       " 'even': 153,\n",
       " 'ever': 154,\n",
       " 'every': 155,\n",
       " 'everyone': 156,\n",
       " 'everything': 157,\n",
       " 'example': 158,\n",
       " 'excellent': 159,\n",
       " 'exciting': 160,\n",
       " 'experience': 161,\n",
       " 'extremely': 162,\n",
       " 'eye': 163,\n",
       " 'face': 164,\n",
       " 'family': 165,\n",
       " 'fan': 166,\n",
       " 'fantastic': 167,\n",
       " 'far': 168,\n",
       " 'fascinating': 169,\n",
       " 'favorite': 170,\n",
       " 'favourite': 171,\n",
       " 'feature': 172,\n",
       " 'feel': 173,\n",
       " 'feeling': 174,\n",
       " 'felt': 175,\n",
       " 'few': 176,\n",
       " 'fighting': 177,\n",
       " 'film': 178,\n",
       " \"film's\": 179,\n",
       " 'filmed': 180,\n",
       " 'final': 181,\n",
       " 'find': 182,\n",
       " 'fine': 183,\n",
       " 'finish': 184,\n",
       " 'first': 185,\n",
       " 'flawless': 186,\n",
       " 'for': 187,\n",
       " 'forward': 188,\n",
       " 'found': 189,\n",
       " 'four': 190,\n",
       " 'franchise': 191,\n",
       " 'french': 192,\n",
       " 'fresh': 193,\n",
       " 'friend': 194,\n",
       " 'frightening': 195,\n",
       " 'from': 196,\n",
       " 'full': 197,\n",
       " 'fun': 198,\n",
       " 'funniest': 199,\n",
       " 'funny': 200,\n",
       " 'future': 201,\n",
       " 'game': 202,\n",
       " 'gave': 203,\n",
       " 'general': 204,\n",
       " 'generation': 205,\n",
       " 'genre': 206,\n",
       " 'genuine': 207,\n",
       " 'get': 208,\n",
       " 'gets': 209,\n",
       " 'giallo': 210,\n",
       " 'girl': 211,\n",
       " 'give': 212,\n",
       " 'gives': 213,\n",
       " 'glad': 214,\n",
       " 'go': 215,\n",
       " 'going': 216,\n",
       " 'good': 217,\n",
       " 'got': 218,\n",
       " 'great': 219,\n",
       " 'greatest': 220,\n",
       " 'gritty': 221,\n",
       " 'guy': 222,\n",
       " 'had': 223,\n",
       " 'hard': 224,\n",
       " 'has': 225,\n",
       " 'hat': 226,\n",
       " 'have': 227,\n",
       " 'having': 228,\n",
       " 'he': 229,\n",
       " 'heard': 230,\n",
       " 'heart': 231,\n",
       " 'hearted': 232,\n",
       " 'heartwarming': 233,\n",
       " 'heavy': 234,\n",
       " 'help': 235,\n",
       " 'her': 236,\n",
       " 'here': 237,\n",
       " 'highest': 238,\n",
       " 'highly': 239,\n",
       " 'hilarious': 240,\n",
       " 'him': 241,\n",
       " 'hip': 242,\n",
       " 'his': 243,\n",
       " 'history': 244,\n",
       " 'hollywood': 245,\n",
       " 'home': 246,\n",
       " 'hope': 247,\n",
       " 'horror': 248,\n",
       " 'house': 249,\n",
       " 'how': 250,\n",
       " 'however': 251,\n",
       " 'humor': 252,\n",
       " 'humour': 253,\n",
       " 'i': 254,\n",
       " 'idea': 255,\n",
       " 'if': 256,\n",
       " 'impossible': 257,\n",
       " 'impressed': 258,\n",
       " 'in': 259,\n",
       " 'incredible': 260,\n",
       " 'incredibly': 261,\n",
       " 'informative': 262,\n",
       " 'innovative': 263,\n",
       " 'inside': 264,\n",
       " 'intense': 265,\n",
       " 'interest': 266,\n",
       " 'interesting': 267,\n",
       " 'into': 268,\n",
       " 'involving': 269,\n",
       " 'is': 270,\n",
       " 'it': 271,\n",
       " 'its': 272,\n",
       " 'jame': 273,\n",
       " 'job': 274,\n",
       " 'just': 275,\n",
       " 'keeps': 276,\n",
       " 'kid': 277,\n",
       " 'killer': 278,\n",
       " 'kind': 279,\n",
       " 'know': 280,\n",
       " 'landmark': 281,\n",
       " 'last': 282,\n",
       " 'late': 283,\n",
       " 'later': 284,\n",
       " 'laugh': 285,\n",
       " 'leading': 286,\n",
       " 'leave': 287,\n",
       " 'let': 288,\n",
       " 'life': 289,\n",
       " 'light': 290,\n",
       " 'like': 291,\n",
       " 'liked': 292,\n",
       " 'likes': 293,\n",
       " 'line': 294,\n",
       " 'little': 295,\n",
       " 'long': 296,\n",
       " 'look': 297,\n",
       " 'looking': 298,\n",
       " 'lost': 299,\n",
       " 'lot': 300,\n",
       " 'love': 301,\n",
       " 'loved': 302,\n",
       " 'lovely': 303,\n",
       " 'loves': 304,\n",
       " 'low': 305,\n",
       " 'made': 306,\n",
       " 'magic': 307,\n",
       " 'main': 308,\n",
       " 'major': 309,\n",
       " 'make': 310,\n",
       " 'makes': 311,\n",
       " 'making': 312,\n",
       " 'man': 313,\n",
       " 'many': 314,\n",
       " 'masterpiece': 315,\n",
       " 'may': 316,\n",
       " 'maybe': 317,\n",
       " 'me': 318,\n",
       " 'meaning': 319,\n",
       " 'melodrama': 320,\n",
       " 'mention': 321,\n",
       " 'might': 322,\n",
       " 'mind': 323,\n",
       " 'minute': 324,\n",
       " 'miss': 325,\n",
       " 'modern': 326,\n",
       " 'modesty': 327,\n",
       " 'money': 328,\n",
       " 'moral': 329,\n",
       " 'more': 330,\n",
       " 'most': 331,\n",
       " 'move': 332,\n",
       " 'movie': 333,\n",
       " 'moving': 334,\n",
       " 'much': 335,\n",
       " 'murder': 336,\n",
       " 'music': 337,\n",
       " 'musical': 338,\n",
       " 'must': 339,\n",
       " 'my': 340,\n",
       " 'myself': 341,\n",
       " 'naive': 342,\n",
       " 'nature': 343,\n",
       " 'nearly': 344,\n",
       " 'need': 345,\n",
       " 'never': 346,\n",
       " 'new': 347,\n",
       " 'nice': 348,\n",
       " 'night': 349,\n",
       " 'no': 350,\n",
       " 'non': 351,\n",
       " 'not': 352,\n",
       " 'novel': 353,\n",
       " 'now': 354,\n",
       " 'number': 355,\n",
       " 'odd': 356,\n",
       " 'of': 357,\n",
       " 'off': 358,\n",
       " 'offered': 359,\n",
       " 'old': 360,\n",
       " 'on': 361,\n",
       " 'once': 362,\n",
       " 'one': 363,\n",
       " 'only': 364,\n",
       " 'opinion': 365,\n",
       " 'or': 366,\n",
       " 'original': 367,\n",
       " 'oscar': 368,\n",
       " 'other': 369,\n",
       " 'our': 370,\n",
       " 'out': 371,\n",
       " 'outstanding': 372,\n",
       " 'over': 373,\n",
       " 'overall': 374,\n",
       " 'own': 375,\n",
       " 'pace': 376,\n",
       " 'part': 377,\n",
       " 'perfect': 378,\n",
       " 'perfectly': 379,\n",
       " 'performance': 380,\n",
       " 'perhaps': 381,\n",
       " 'period': 382,\n",
       " 'person': 383,\n",
       " 'personality': 384,\n",
       " 'picture': 385,\n",
       " 'piece': 386,\n",
       " 'place': 387,\n",
       " 'playing': 388,\n",
       " 'plenty': 389,\n",
       " 'plot': 390,\n",
       " 'poignant': 391,\n",
       " 'point': 392,\n",
       " 'positive': 393,\n",
       " 'power': 394,\n",
       " 'powerful': 395,\n",
       " 'pretty': 396,\n",
       " 'principal': 397,\n",
       " 'probably': 398,\n",
       " 'production': 399,\n",
       " 'protagonist': 400,\n",
       " 'pure': 401,\n",
       " 'put': 402,\n",
       " 'quality': 403,\n",
       " 'quickly': 404,\n",
       " 'quite': 405,\n",
       " 'rare': 406,\n",
       " 'rate': 407,\n",
       " 'rather': 408,\n",
       " 're': 409,\n",
       " 'read': 410,\n",
       " 'real': 411,\n",
       " 'realise': 412,\n",
       " 'realistic': 413,\n",
       " 'really': 414,\n",
       " 'recommend': 415,\n",
       " 'recommended': 416,\n",
       " 'released': 417,\n",
       " 'rent': 418,\n",
       " 'revenge': 419,\n",
       " 'richard': 420,\n",
       " 'ride': 421,\n",
       " 'road': 422,\n",
       " 'role': 423,\n",
       " 'romance': 424,\n",
       " 'running': 425,\n",
       " 'said': 426,\n",
       " 'same': 427,\n",
       " 'samurai': 428,\n",
       " 'saw': 429,\n",
       " 'say': 430,\n",
       " 'scene': 431,\n",
       " 'score': 432,\n",
       " 'screen': 433,\n",
       " 'screenplay': 434,\n",
       " 'script': 435,\n",
       " 'scripted': 436,\n",
       " 'season': 437,\n",
       " 'second': 438,\n",
       " 'see': 439,\n",
       " 'seeing': 440,\n",
       " 'seen': 441,\n",
       " 'sense': 442,\n",
       " 'sequence': 443,\n",
       " 'series': 444,\n",
       " 'set': 445,\n",
       " 'several': 446,\n",
       " 'sfx': 447,\n",
       " 'sharp': 448,\n",
       " 'she': 449,\n",
       " 'sheer': 450,\n",
       " 'short': 451,\n",
       " 'shot': 452,\n",
       " 'should': 453,\n",
       " 'show': 454,\n",
       " 'simple': 455,\n",
       " 'simply': 456,\n",
       " 'since': 457,\n",
       " 'slasher': 458,\n",
       " 'slow': 459,\n",
       " 'smallville': 460,\n",
       " 'smile': 461,\n",
       " 'so': 462,\n",
       " 'society': 463,\n",
       " 'some': 464,\n",
       " 'someone': 465,\n",
       " 'something': 466,\n",
       " 'somewhat': 467,\n",
       " 'song': 468,\n",
       " 'soon': 469,\n",
       " 'sound': 470,\n",
       " 'sounds': 471,\n",
       " 'soundtrack': 472,\n",
       " 'space': 473,\n",
       " 'special': 474,\n",
       " 'spirited': 475,\n",
       " 'spite': 476,\n",
       " 'sport': 477,\n",
       " 'star': 478,\n",
       " 'start': 479,\n",
       " 'still': 480,\n",
       " 'stop': 481,\n",
       " 'story': 482,\n",
       " 'strange': 483,\n",
       " 'strong': 484,\n",
       " 'strongly': 485,\n",
       " 'study': 486,\n",
       " 'stylish': 487,\n",
       " 'successful': 488,\n",
       " 'such': 489,\n",
       " 'suffering': 490,\n",
       " 'suggest': 491,\n",
       " 'superb': 492,\n",
       " 'superior': 493,\n",
       " 'supporting': 494,\n",
       " 'supposed': 495,\n",
       " 'sure': 496,\n",
       " 'surprise': 497,\n",
       " 'surprised': 498,\n",
       " 'surprisingly': 499,\n",
       " 'sweet': 500,\n",
       " 'swept': 501,\n",
       " 'take': 502,\n",
       " 'takes': 503,\n",
       " 'taking': 504,\n",
       " 'tale': 505,\n",
       " 'tape': 506,\n",
       " 'television': 507,\n",
       " 'ten': 508,\n",
       " 'terrific': 509,\n",
       " 'than': 510,\n",
       " 'thank': 511,\n",
       " 'that': 512,\n",
       " 'the': 513,\n",
       " 'their': 514,\n",
       " 'them': 515,\n",
       " 'then': 516,\n",
       " 'there': 517,\n",
       " 'these': 518,\n",
       " 'they': 519,\n",
       " 'thing': 520,\n",
       " 'think': 521,\n",
       " 'this': 522,\n",
       " 'thoroughly': 523,\n",
       " 'those': 524,\n",
       " 'though': 525,\n",
       " 'thought': 526,\n",
       " 'three': 527,\n",
       " 'thriller': 528,\n",
       " 'through': 529,\n",
       " 'tightly': 530,\n",
       " 'time': 531,\n",
       " 'to': 532,\n",
       " 'today': 533,\n",
       " \"today's\": 534,\n",
       " 'too': 535,\n",
       " 'top': 536,\n",
       " 'touching': 537,\n",
       " 'tough': 538,\n",
       " 'toward': 539,\n",
       " 'treat': 540,\n",
       " 'trilogy': 541,\n",
       " 'true': 542,\n",
       " 'truly': 543,\n",
       " 'tv': 544,\n",
       " 'twist': 545,\n",
       " 'two': 546,\n",
       " 'type': 547,\n",
       " 'typical': 548,\n",
       " 'under': 549,\n",
       " 'understand': 550,\n",
       " 'unique': 551,\n",
       " 'until': 552,\n",
       " 'up': 553,\n",
       " 'us': 554,\n",
       " 'used': 555,\n",
       " 'value': 556,\n",
       " 'version': 557,\n",
       " 'very': 558,\n",
       " 'viewing': 559,\n",
       " 'vote': 560,\n",
       " 'want': 561,\n",
       " 'wanted': 562,\n",
       " 'war': 563,\n",
       " 'warm': 564,\n",
       " 'was': 565,\n",
       " 'watch': 566,\n",
       " 'watched': 567,\n",
       " 'watching': 568,\n",
       " 'way': 569,\n",
       " 'we': 570,\n",
       " 'well': 571,\n",
       " 'went': 572,\n",
       " 'were': 573,\n",
       " 'western': 574,\n",
       " 'what': 575,\n",
       " 'when': 576,\n",
       " 'where': 577,\n",
       " 'whether': 578,\n",
       " 'which': 579,\n",
       " 'while': 580,\n",
       " 'white': 581,\n",
       " 'who': 582,\n",
       " 'whole': 583,\n",
       " 'why': 584,\n",
       " 'will': 585,\n",
       " 'winner': 586,\n",
       " 'with': 587,\n",
       " 'without': 588,\n",
       " 'wonderful': 589,\n",
       " 'word': 590,\n",
       " 'work': 591,\n",
       " 'world': 592,\n",
       " 'worth': 593,\n",
       " 'would': 594,\n",
       " 'writer': 595,\n",
       " 'writing': 596,\n",
       " 'written': 597,\n",
       " 'wrong': 598,\n",
       " 'year': 599,\n",
       " 'yes': 600,\n",
       " 'yet': 601,\n",
       " 'you': 602,\n",
       " 'your': 603,\n",
       " 'yourself': 604}"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1864,)\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "token_pattern = re.compile(token)\n",
    "X = []\n",
    "i=0\n",
    "for sentence in X_stack:\n",
    "    split = token_pattern.findall(sentence)\n",
    "    seq = []\n",
    "    for word in split:\n",
    "        try:\n",
    "            seq.append(word_dict[word])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    X.append(seq)\n",
    "    \n",
    "X = np.asarray(X)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_stack[260]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_stack, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (1248, 100)\n",
      "x_test shape: (616, 100)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "maxlen=100\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605\n"
     ]
    }
   ],
   "source": [
    "dict_len = len(tf_vectorizer.get_feature_names())\n",
    "batch_size = 32\n",
    "print(dict_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anneke Hidayat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\Anneke Hidayat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(input_shape=(None, 500..., units=10)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(dict_len, 500))\n",
    "model.add(LSTM(output_dim=hidden_neurons, input_dim=500))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, None, 500)         302500    \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 10)                20440     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 322,951\n",
      "Trainable params: 322,951\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Epoch 1/50\n",
      "1248/1248 [==============================] - 5s 4ms/step - loss: 0.5952\n",
      "Epoch 2/50\n",
      "1248/1248 [==============================] - 5s 4ms/step - loss: 0.5956\n",
      "Epoch 3/50\n",
      "1248/1248 [==============================] - 6s 4ms/step - loss: 0.5946\n",
      "Epoch 4/50\n",
      "1248/1248 [==============================] - 6s 5ms/step - loss: 0.5947\n",
      "Epoch 5/50\n",
      "1248/1248 [==============================] - 7s 5ms/step - loss: 0.5942\n",
      "Epoch 6/50\n",
      "1248/1248 [==============================] - 7s 5ms/step - loss: 0.5945\n",
      "Epoch 7/50\n",
      "1248/1248 [==============================] - 7s 6ms/step - loss: 0.5943\n",
      "Epoch 8/50\n",
      "1248/1248 [==============================] - 7s 6ms/step - loss: 0.5944\n",
      "Epoch 9/50\n",
      "1248/1248 [==============================] - 8s 6ms/step - loss: 0.5939\n",
      "Epoch 10/50\n",
      "1248/1248 [==============================] - 7s 6ms/step - loss: 0.5947\n",
      "Epoch 11/50\n",
      "1248/1248 [==============================] - 7s 5ms/step - loss: 0.5952\n",
      "Epoch 12/50\n",
      "1248/1248 [==============================] - 7s 5ms/step - loss: 0.5945\n",
      "Epoch 13/50\n",
      "1248/1248 [==============================] - 6s 5ms/step - loss: 0.5944\n",
      "Epoch 14/50\n",
      "1248/1248 [==============================] - 5s 4ms/step - loss: 0.5944\n",
      "Epoch 15/50\n",
      "1248/1248 [==============================] - 5s 4ms/step - loss: 0.5942\n",
      "Epoch 16/50\n",
      "1248/1248 [==============================] - 5s 4ms/step - loss: 0.5940\n",
      "Epoch 17/50\n",
      "1248/1248 [==============================] - 5s 4ms/step - loss: 0.5944\n",
      "Epoch 18/50\n",
      "1248/1248 [==============================] - 5s 4ms/step - loss: 0.5936\n",
      "Epoch 19/50\n",
      "1248/1248 [==============================] - 5s 4ms/step - loss: 0.5941\n",
      "Epoch 20/50\n",
      "1248/1248 [==============================] - 5s 4ms/step - loss: 0.5945\n",
      "Epoch 21/50\n",
      "1248/1248 [==============================] - 5s 4ms/step - loss: 0.5943\n",
      "Epoch 22/50\n",
      "1248/1248 [==============================] - 5s 4ms/step - loss: 0.5942\n",
      "Epoch 23/50\n",
      "1248/1248 [==============================] - 5s 4ms/step - loss: 0.5940\n",
      "Epoch 24/50\n",
      "1248/1248 [==============================] - 6s 5ms/step - loss: 0.5945\n",
      "Epoch 25/50\n",
      "1248/1248 [==============================] - 6s 5ms/step - loss: 0.5938\n",
      "Epoch 26/50\n",
      "1248/1248 [==============================] - 6s 5ms/step - loss: 0.5937\n",
      "Epoch 27/50\n",
      "1248/1248 [==============================] - 6s 4ms/step - loss: 0.5936\n",
      "Epoch 28/50\n",
      "1248/1248 [==============================] - 7s 5ms/step - loss: 0.5940\n",
      "Epoch 29/50\n",
      "1248/1248 [==============================] - 6s 5ms/step - loss: 0.5934\n",
      "Epoch 30/50\n",
      "1248/1248 [==============================] - 7s 5ms/step - loss: 0.5940\n",
      "Epoch 31/50\n",
      "1248/1248 [==============================] - 6s 5ms/step - loss: 0.5937\n",
      "Epoch 32/50\n",
      "1248/1248 [==============================] - 8s 7ms/step - loss: 0.5934\n",
      "Epoch 33/50\n",
      "1248/1248 [==============================] - 8s 6ms/step - loss: 0.5934\n",
      "Epoch 34/50\n",
      "1248/1248 [==============================] - 7s 6ms/step - loss: 0.5941\n",
      "Epoch 35/50\n",
      "1248/1248 [==============================] - 6s 5ms/step - loss: 0.5948\n",
      "Epoch 36/50\n",
      "1248/1248 [==============================] - 6s 5ms/step - loss: 0.5936\n",
      "Epoch 37/50\n",
      "1248/1248 [==============================] - 7s 5ms/step - loss: 0.5935\n",
      "Epoch 38/50\n",
      "1248/1248 [==============================] - 6s 5ms/step - loss: 0.5936\n",
      "Epoch 39/50\n",
      "1248/1248 [==============================] - 5s 4ms/step - loss: 0.5932\n",
      "Epoch 40/50\n",
      "1248/1248 [==============================] - 5s 4ms/step - loss: 0.5932\n",
      "Epoch 41/50\n",
      "1248/1248 [==============================] - 5s 4ms/step - loss: 0.5932\n",
      "Epoch 42/50\n",
      "1248/1248 [==============================] - 5s 4ms/step - loss: 0.5932\n",
      "Epoch 43/50\n",
      "1248/1248 [==============================] - 5s 4ms/step - loss: 0.5941\n",
      "Epoch 44/50\n",
      "1248/1248 [==============================] - 5s 4ms/step - loss: 0.5932\n",
      "Epoch 45/50\n",
      "1248/1248 [==============================] - 5s 4ms/step - loss: 0.5935\n",
      "Epoch 46/50\n",
      "1248/1248 [==============================] - 5s 4ms/step - loss: 0.5930\n",
      "Epoch 47/50\n",
      "1248/1248 [==============================] - 5s 4ms/step - loss: 0.5935\n",
      "Epoch 48/50\n",
      "1248/1248 [==============================] - 5s 4ms/step - loss: 0.5938\n",
      "Epoch 49/50\n",
      "1248/1248 [==============================] - 6s 5ms/step - loss: 0.5930\n",
      "Epoch 50/50\n",
      "1248/1248 [==============================] - 6s 5ms/step - loss: 0.5933\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Train...')\n",
    "hist = model.fit(x_train, y_train, epochs=50, verbose=1, callbacks=[tensorboard])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.callbacks.History object at 0x000001CEA690B780>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21915584415584416"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_predict = model.predict(x_test)\n",
    "\n",
    "y = []\n",
    "\n",
    "for pred in y_predict:\n",
    "    if pred > 0.5:\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(0)\n",
    "\n",
    "        \n",
    "np.sum(y_test == y)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_neurons = voc \n",
    "\n",
    "hidden_neurons = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anneke Hidayat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "C:\\Users\\Anneke Hidayat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(input_shape=(None, 368..., units=10)`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 10)                147840    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 147,851\n",
      "Trainable params: 147,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "# model.add(Embedding(voc, 32, input_length=max_length))\n",
    "# model.add(Flatten())\n",
    "model.add(LSTM(output_dim=hidden_neurons, input_dim=in_neurons))\n",
    "\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'], class_mode=\"binary\")\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
