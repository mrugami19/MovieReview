{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_imdb(path, shuffle=True, random_state=42):\n",
    "    import glob \n",
    "    print(\"Loading the imdb data\")\n",
    "    \n",
    "    train_neg_files = glob.glob(path+\"/train/neg/*.txt\")\n",
    "    train_pos_files = glob.glob(path+\"/train/pos/*.txt\")\n",
    "    \n",
    "    X_train_corpus = []\n",
    "    y_train = []\n",
    "    \n",
    "    for tnf in train_neg_files:\n",
    "        f = open(tnf, 'r', encoding=\"utf8\")\n",
    "        line = f.read()\n",
    "        #line = line[:len(line)/2]\n",
    "        X_train_corpus.append(line)\n",
    "        y_train.append(0)\n",
    "        f.close()\n",
    "    \n",
    "    for tpf in train_pos_files:\n",
    "        f = open(tpf, 'r', encoding=\"utf8\")\n",
    "        line = f.read()\n",
    "        #line = line[:len(line)/2]\n",
    "        X_train_corpus.append(line)\n",
    "        y_train.append(1)\n",
    "        f.close()\n",
    "    \n",
    "    print(\"Train Data loaded.\")\n",
    "    \n",
    "    test_neg_files = glob.glob(path+\"/test/neg/*.txt\")\n",
    "    test_pos_files = glob.glob(path+\"/test/pos/*.txt\")\n",
    "    \n",
    "    X_test_corpus = []\n",
    "    y_test = []\n",
    "    \n",
    "    for tnf in test_neg_files:\n",
    "        f = open(tnf, 'r', encoding=\"utf8\")\n",
    "        X_test_corpus.append(f.read())\n",
    "        y_test.append(0)\n",
    "        f.close()\n",
    "    \n",
    "    for tpf in test_pos_files:\n",
    "        f = open(tpf, 'r', encoding=\"utf8\")\n",
    "        X_test_corpus.append(f.read())\n",
    "        y_test.append(1)\n",
    "        f.close()\n",
    "    \n",
    "    print(\"Test Data loaded.\")\n",
    "    \n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.seed(random_state)\n",
    "        indices = np.random.permutation(len(y_train))       \n",
    "        \n",
    "        #X_train = X_train.tocsr()\n",
    "        #X_train_corpus = X_train_corpus[indices]\n",
    "        X_train_corpus = [X_train_corpus[i] for i in indices]\n",
    "        y_train = y_train[indices]\n",
    "        #train_corpus_shuffled = [train_corpus[i] for i in indices]\n",
    "        \n",
    "        indices = np.random.permutation(len(y_test))\n",
    "        \n",
    "        #X_test = X_test.tocsr()\n",
    "        #X_test_corpus = X_test_corpus[indices]\n",
    "        X_test_corpus = [X_test_corpus[i] for i in indices]\n",
    "        y_test = y_test[indices]\n",
    "        #test_corpus_shuffled = [test_corpus[i] for i in indices]\n",
    "    #else:\n",
    "        #train_corpus_shuffled = train_corpus\n",
    "        #test_corpus_shuffled = test_corpus\n",
    "    \n",
    "    return X_train_corpus, y_train, X_test_corpus , y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "'''\n",
    "Read and load the contraction list (or any text files)\n",
    "'''\n",
    "def load_list(filename, split_delimiter):\n",
    "    vocabulary = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for l in f:\n",
    "            vocabulary.append(l.strip().split(split_delimiter))\n",
    "    return np.asarray(vocabulary)\n",
    "\n",
    "'''\n",
    "Clean the HTML tags from the corpus\n",
    "'''\n",
    "def cleanhtml(text):\n",
    "#     cleanr = re.compile('<.*?>')\n",
    "#     cleantag = re.sub(cleanr, '', text)\n",
    "    cleantag = re.sub(re.compile('<.*?>'), '', text)\n",
    "    cleantext = cleantag.replace('br', '')\n",
    "    return cleantext\n",
    "\n",
    "# Reference :\n",
    "# https://stackoverflow.com/questions/9662346/python-code-to-remove-html-tags-from-a-string\n",
    "\n",
    "'''\n",
    "Replace the contraction words into two parts (by given contraction list)\n",
    "'''\n",
    "def replace_contraction(corpus, cont_list):\n",
    "    for i in range(0, cont_list.shape[0]):\n",
    "        corpus = corpus.lower().replace(cont_list[i,0], cont_list[i,1])\n",
    "    return corpus\n",
    "\n",
    "'''\n",
    "Singularize the words by its POS-tag\n",
    "'''\n",
    "def word_singularize(corpus):\n",
    "    from textblob import TextBlob\n",
    "    \n",
    "    text = TextBlob(corpus)\n",
    "    for tag in text.tags:\n",
    "        if tag[1] == 'NNS' and tag[0] != 'yes':\n",
    "            corpus = corpus.replace(tag[0], tag[0].singularize())\n",
    "    return corpus\n",
    "\n",
    "'''\n",
    "Update clean corpus\n",
    "'''\n",
    "def update_corpus_contraction(X_corpus):\n",
    "    cont_list = load_list(\"contraction_list.txt\", ',')\n",
    "    print(cont_list.shape)\n",
    "    print('corpus update start')\n",
    "    for i in range(0,len(X_corpus)):\n",
    "        X_corpus[i] = cleanhtml(X_corpus[i])\n",
    "        X_corpus[i] = replace_contraction(X_corpus[i], cont_list)\n",
    "        X_corpus[i] = word_singularize(X_corpus[i])\n",
    "        X_corpus[i] = X_corpus[i].replace('&', 'and')\n",
    "    print('corpus update end')\n",
    "    print()\n",
    "    return X_corpus\n",
    "\n",
    "'''\n",
    "Count the negative and positive frequency\n",
    "'''\n",
    "def negative_positive_counts(X, y, word_index):\n",
    "    neg_count = np.sum(X[y==0, word_index])\n",
    "    pos_count = np.sum(X[y==1, word_index])    \n",
    "    return neg_count, pos_count\n",
    "\n",
    "'''\n",
    "Count the ratio : log(#pos/#neg)\n",
    "'''\n",
    "def log_ratio_positive_negative(X, y, word_index):\n",
    "    neg_count, pos_count = negative_positive_counts(X,y, word_index)\n",
    "    log_ratio = np.log(pos_count+1)-np.log(neg_count+1)\n",
    "    return log_ratio, neg_count, pos_count\n",
    "\n",
    "'''\n",
    "Sort top words w.r.t log ratio and write into file\n",
    "'''\n",
    "def sort_top_words_with_count(X, y, words,filename, top_k=10):\n",
    "    log_ratio = []\n",
    "    neg_count = []\n",
    "    pos_count = []\n",
    "    \n",
    "    for i in range(0,len(words)):\n",
    "        log_ratio_, neg_count_, pos_count_ = log_ratio_positive_negative(X, y, i)\n",
    "        log_ratio.append(log_ratio_)\n",
    "        neg_count.append(neg_count_)\n",
    "        pos_count.append(pos_count_)\n",
    "    \n",
    "    sorted_indices_descending_abs = np.argsort(np.absolute(log_ratio))[::-1]\n",
    "    \n",
    "    filename = filename + '.txt'\n",
    "    with open(filename, mode='w', encoding='utf8') as w:\n",
    "        for i in sorted_indices_descending_abs[: top_k]:\n",
    "#             print(\"%s\\t%0.2f\" %(words[i], weights[i]))\n",
    "#             n_p=negative_positive_counts(X, y, i)\n",
    "            w.write(\"%s\\t%0.2f\\t%d\\t%d\" %(str(words[i]), log_ratio[i], neg_count[i], pos_count[i]))\n",
    "            w.write('\\n')\n",
    "        w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Anne Soraya\\Documents\\IIT_resources\\Python\\aclImdb\"\n",
    "# X_train_corpus , y_train, X_test_corpus , y_test = load_imdb(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_corpus_update = update_corpus_contraction(X_train_corpus)\n",
    "# X_test_corpus_update = update_corpus_contraction(X_test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(path, X):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(X, f)\n",
    "def open_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    return X\n",
    "    \n",
    "# save_pickle(\"./pickles/imdb_X_tr_corpus.pickle\", X_train_corpus)\n",
    "# save_pickle(\"./pickles/imdb_y__tr_corpus.pickle\", y_train)\n",
    "# save_pickle(\"./pickles/imdb_x_tr_clean.pickle\", X_train_corpus_update)\n",
    "X_tr_clean = open_pickle(\"./pickles/imdb_x_tr_clean.pickle\")\n",
    "y_tr = open_pickle(\"./pickles/imdb_y_tr.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[106, 409, 434, 488, 520, 711, 750, 1139, 1169, 1291, 1307, 1424, 1519, 1535, 1584, 1654, 1674, 1679, 1824, 2045, 2286, 2547, 2615, 2664, 2677, 2803, 3150, 3257, 3432, 3527, 3582, 3611, 3657, 3733, 3811, 3814, 4010, 4012, 4333, 4374, 4506, 4552, 4554, 4557, 4741, 4803, 5514, 5574, 5635, 5820, 5881, 5925, 5977, 6065, 6201, 6216, 6224, 6227, 6873, 6912, 6924, 7359, 7428, 7527, 7573, 8279, 8751, 8785, 8928, 8935, 9044, 9195, 9459, 9654, 9674, 9863, 9891, 9980, 10133, 10299, 10403, 10415, 10476, 10617, 10647, 10834, 10980, 11087, 11199, 11438, 11490, 11498, 11543, 11731, 11946, 11955, 12066, 12135, 12149, 12432]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "record the positive and negative indices\n",
    "'''\n",
    "\n",
    "y_pos_indices = np.asarray(np.where(y_tr == 1)).reshape(12500)\n",
    "y_neg_indices = np.asarray(np.where(y_tr == 0)).reshape(12500)\n",
    "\n",
    "'''\n",
    "Make a random indices (0,12500) s.t it distribute among the corpus\n",
    "'''\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "rand = random.sample(range(0, 12500), 100)\n",
    "rand_indices = sorted(rand)\n",
    "print(rand_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "216\n",
      "label :  1\n",
      "0 : if the lion king was a disney version of hamlet, then the lion king 3: hakuna matata is a disney version of guildenstern and rosencrantz are dead.\n",
      "1 : just like tom stoppard's beguiling film, we get to view the action from the point of view of two of the minor character from the original: timon, the meerkat with a penchant for eaking into song at the drop of a hat, and pumbaa, the warthog with flatulence issue.\n",
      "2 : by following their story - rather than simba's - we get to see why all the animal bowed down as simba was presented from pride rock.\n",
      "3 : we find out what made timon and pumbaa decide to follow simba back to pride rock to oust scar.\n",
      "4 : and we find out how they dealt with the hyena's once and for all.\n",
      "5 : nathan lane as timon gets most of the best joke, but he is ably supported by ernie sabella as pumbaa.\n",
      "6 : it is also good to hear matthew broderick and whoopi goldberg reprising their role.\n",
      "7 : julie kavner and jerry stiller lend their distinctive voice to two new character: timon's mother and uncle.\n",
      "8 : the only downside is the constant stop-start-rewind-fast-forward device which does not always help to progress the story.\n",
      "9 : having said that, there is a illiant zoom near the beginning of the movie.\n",
      "10 : with more laugh than any other third-in-a-disney-series movie, hakuna matata is worth watching - if only for the hot tub scene which is still funny despite being a little bit predictable.\n"
     ]
    }
   ],
   "source": [
    "def print_sentence(corpus):\n",
    "    from textblob import TextBlob\n",
    "    text = TextBlob(corpus)\n",
    "    i = 0\n",
    "    for sentence in text.sentences:\n",
    "        print(i, ':', sentence)\n",
    "        i += 1\n",
    "\n",
    "print(rand_indices[0])\n",
    "print('corpus index : ', y_pos_indices[rand_indices[0]])\n",
    "print('label : ', y_tr[y_pos_indices[rand_indices[0]]])\n",
    "print_sentence(X_tr_clean[y_pos_indices[rand_indices[0]]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
