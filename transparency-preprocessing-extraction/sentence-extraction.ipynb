{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_imdb(path, shuffle=True, random_state=42):\n",
    "    import glob \n",
    "    print(\"Loading the imdb data\")\n",
    "    \n",
    "    train_neg_files = glob.glob(path+\"/train/neg/*.txt\")\n",
    "    train_pos_files = glob.glob(path+\"/train/pos/*.txt\")\n",
    "    \n",
    "    X_train_corpus = []\n",
    "    y_train = []\n",
    "    \n",
    "    for tnf in train_neg_files:\n",
    "        f = open(tnf, 'r', encoding=\"utf8\")\n",
    "        line = f.read()\n",
    "        #line = line[:len(line)/2]\n",
    "        X_train_corpus.append(line)\n",
    "        y_train.append(0)\n",
    "        f.close()\n",
    "    \n",
    "    for tpf in train_pos_files:\n",
    "        f = open(tpf, 'r', encoding=\"utf8\")\n",
    "        line = f.read()\n",
    "        #line = line[:len(line)/2]\n",
    "        X_train_corpus.append(line)\n",
    "        y_train.append(1)\n",
    "        f.close()\n",
    "    \n",
    "    print(\"Train Data loaded.\")\n",
    "    \n",
    "    test_neg_files = glob.glob(path+\"/test/neg/*.txt\")\n",
    "    test_pos_files = glob.glob(path+\"/test/pos/*.txt\")\n",
    "    \n",
    "    X_test_corpus = []\n",
    "    y_test = []\n",
    "    \n",
    "    for tnf in test_neg_files:\n",
    "        f = open(tnf, 'r', encoding=\"utf8\")\n",
    "        X_test_corpus.append(f.read())\n",
    "        y_test.append(0)\n",
    "        f.close()\n",
    "    \n",
    "    for tpf in test_pos_files:\n",
    "        f = open(tpf, 'r', encoding=\"utf8\")\n",
    "        X_test_corpus.append(f.read())\n",
    "        y_test.append(1)\n",
    "        f.close()\n",
    "    \n",
    "    print(\"Test Data loaded.\")\n",
    "    \n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.seed(random_state)\n",
    "        indices = np.random.permutation(len(y_train))       \n",
    "        \n",
    "        #X_train = X_train.tocsr()\n",
    "        #X_train_corpus = X_train_corpus[indices]\n",
    "        X_train_corpus = [X_train_corpus[i] for i in indices]\n",
    "        y_train = y_train[indices]\n",
    "        #train_corpus_shuffled = [train_corpus[i] for i in indices]\n",
    "        \n",
    "        indices = np.random.permutation(len(y_test))\n",
    "        \n",
    "        #X_test = X_test.tocsr()\n",
    "        #X_test_corpus = X_test_corpus[indices]\n",
    "        X_test_corpus = [X_test_corpus[i] for i in indices]\n",
    "        y_test = y_test[indices]\n",
    "        #test_corpus_shuffled = [test_corpus[i] for i in indices]\n",
    "    #else:\n",
    "        #train_corpus_shuffled = train_corpus\n",
    "        #test_corpus_shuffled = test_corpus\n",
    "    \n",
    "    return X_train_corpus, y_train, X_test_corpus , y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "'''\n",
    "Read and load the contraction list (or any text files)\n",
    "'''\n",
    "def load_list(filename, split_delimiter):\n",
    "    vocabulary = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for l in f:\n",
    "            vocabulary.append(l.strip().split(split_delimiter))\n",
    "    return np.asarray(vocabulary)\n",
    "\n",
    "'''\n",
    "Clean the HTML tags from the corpus\n",
    "'''\n",
    "def cleanhtml(text):\n",
    "#     cleanr = re.compile('<.*?>')\n",
    "#     cleantag = re.sub(cleanr, '', text)\n",
    "    cleantag = re.sub(re.compile('<.*?>'), '', text)\n",
    "#     cleantext = cleantag.replace('br', '')\n",
    "    return cleantag\n",
    "\n",
    "# Reference :\n",
    "# https://stackoverflow.com/questions/9662346/python-code-to-remove-html-tags-from-a-string\n",
    "\n",
    "'''\n",
    "Replace the contraction words into two parts (by given contraction list)\n",
    "'''\n",
    "def replace_contraction(corpus, cont_list):\n",
    "    for i in range(0, cont_list.shape[0]):\n",
    "        corpus = corpus.lower().replace(cont_list[i,0], cont_list[i,1])\n",
    "    return corpus\n",
    "\n",
    "'''\n",
    "Singularize the words by its POS-tag\n",
    "'''\n",
    "def word_singularize(corpus):\n",
    "    from textblob import TextBlob\n",
    "    \n",
    "    text = TextBlob(corpus)\n",
    "    for tag in text.tags:\n",
    "        if tag[1] == 'NNS' and tag[0] != 'yes':\n",
    "            corpus = corpus.replace(tag[0], tag[0].singularize())\n",
    "    return corpus\n",
    "\n",
    "'''\n",
    "Update clean corpus\n",
    "'''\n",
    "def update_corpus_contraction(X_corpus):\n",
    "    cont_list = load_list(\"contraction_list.txt\", ',')\n",
    "    print(cont_list.shape)\n",
    "    print('corpus update start')\n",
    "    for i in range(0,len(X_corpus)):\n",
    "        X_corpus[i] = cleanhtml(X_corpus[i])\n",
    "        X_corpus[i] = replace_contraction(X_corpus[i], cont_list)\n",
    "        X_corpus[i] = word_singularize(X_corpus[i])\n",
    "        X_corpus[i] = X_corpus[i].replace('&', 'and')\n",
    "    print('corpus update end')\n",
    "    print()\n",
    "    return X_corpus\n",
    "\n",
    "'''\n",
    "Count the negative and positive frequency\n",
    "'''\n",
    "def negative_positive_counts(X, y, word_index):\n",
    "    neg_count = np.sum(X[y==0, word_index])\n",
    "    pos_count = np.sum(X[y==1, word_index])    \n",
    "    return neg_count, pos_count\n",
    "\n",
    "'''\n",
    "Count the ratio : log(#pos/#neg)\n",
    "'''\n",
    "def log_ratio_positive_negative(X, y, word_index):\n",
    "    neg_count, pos_count = negative_positive_counts(X,y, word_index)\n",
    "    log_ratio = np.log(pos_count+1)-np.log(neg_count+1)\n",
    "    return log_ratio, neg_count, pos_count\n",
    "\n",
    "'''\n",
    "Sort top words w.r.t log ratio and write into file\n",
    "'''\n",
    "def sort_top_words_with_count(X, y, words,filename, top_k=10):\n",
    "    log_ratio = []\n",
    "    neg_count = []\n",
    "    pos_count = []\n",
    "    \n",
    "    for i in range(0,len(words)):\n",
    "        log_ratio_, neg_count_, pos_count_ = log_ratio_positive_negative(X, y, i)\n",
    "        log_ratio.append(log_ratio_)\n",
    "        neg_count.append(neg_count_)\n",
    "        pos_count.append(pos_count_)\n",
    "    \n",
    "    sorted_indices_descending_abs = np.argsort(np.absolute(log_ratio))[::-1]\n",
    "    \n",
    "    filename = filename + '.txt'\n",
    "    with open(filename, mode='w', encoding='utf8') as w:\n",
    "        for i in sorted_indices_descending_abs[: top_k]:\n",
    "#             print(\"%s\\t%0.2f\" %(words[i], weights[i]))\n",
    "#             n_p=negative_positive_counts(X, y, i)\n",
    "            w.write(\"%s\\t%0.2f\\t%d\\t%d\" %(str(words[i]), log_ratio[i], neg_count[i], pos_count[i]))\n",
    "            w.write('\\n')\n",
    "        w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the imdb data\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "# X_train_corpus_update = update_corpus_contraction(X_tr)\n",
    "X_test_corpus_update = update_corpus_contraction(X_tr)"
=======
    "path = r\"C:\\Users\\Anne Soraya\\Documents\\IIT_resources\\Python\\aclImdb\"\n",
    "X_train_corpus , y_train, X_test_corpus , y_test = load_imdb(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_corpus_update = update_corpus_contraction(X_tr)\n",
    "X_test_corpus_update = update_corpus_contraction(X_test)"
>>>>>>> b328c2988fb62a64ae37029a122dfc2d8a78b318
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_pickle(path, X):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(X, f)\n",
    "def open_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    return X\n",
    "    \n",
    "# save_pickle(\"./pickles/imdb_X_tr_corpus.pickle\", X_train_corpus)\n",
    "# save_pickle(\"./pickles/imdb_y_tr_corpus.pickle\", y_train)\n",
    "# save_pickle(\"./pickles/imdb_x_tr_clean.pickle\", X_train_corpus_update)\n",
    "save_pickle(\"./pickles/imdb_x_te_clean.pickle\", X_test_corpus_update)\n",
    "save_pickle(\"./pickles/imdb_X_te_corpus.pickle\", X_test_corpus)\n",
    "save_pickle(\"./pickles/imdb_y_te.pickle\", y_test)\n",
    "X_tr = open_pickle(\"./pickles/imdb_X_tr_corpus.pickle\")\n",
    "X_tr_clean = open_pickle(\"./pickles/imdb_x_tr_clean.pickle\")\n",
    "y_tr = open_pickle(\"./pickles/imdb_y_tr.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "72\n",
      "111\n",
      "116\n",
      "205\n",
      "232\n",
      "246\n",
      "294\n",
      "306\n",
      "357\n",
      "455\n",
      "608\n",
      "650\n",
      "769\n",
      "774\n",
      "823\n",
      "945\n",
      "1010\n",
      "1024\n",
      "1033\n",
      "1158\n",
      "1243\n",
      "1275\n",
      "1282\n",
      "1344\n",
      "1396\n",
      "1411\n",
      "1464\n",
      "1503\n",
      "1583\n",
      "1592\n",
      "1596\n",
      "1701\n",
      "1751\n",
      "1778\n",
      "1784\n",
      "1815\n",
      "1830\n",
      "1834\n",
      "1841\n",
      "1894\n",
      "1906\n",
      "2000\n",
      "2012\n",
      "2084\n",
      "2119\n",
      "2130\n",
      "2146\n",
      "2156\n",
      "2165\n",
      "2179\n",
      "2223\n",
      "2231\n",
      "2240\n",
      "2274\n",
      "2298\n",
      "2357\n",
      "2363\n",
      "2478\n",
      "2480\n",
      "2517\n",
      "2523\n",
      "2587\n",
      "2689\n",
      "2727\n",
      "2859\n",
      "2895\n",
      "2910\n",
      "2933\n",
      "2946\n",
      "2975\n",
      "3069\n",
      "3075\n",
      "3116\n",
      "3128\n",
      "3186\n",
      "3215\n",
      "3247\n",
      "3258\n",
      "3274\n",
      "3296\n",
      "3406\n",
      "3428\n",
      "3456\n",
      "3503\n",
      "3513\n",
      "3571\n",
      "3581\n",
      "3597\n",
      "3681\n",
      "3709\n",
      "3858\n",
      "3885\n",
      "4031\n",
      "4060\n",
      "4131\n",
      "4176\n",
      "4249\n",
      "4251\n",
      "4294\n",
      "4298\n",
      "4351\n",
      "4493\n",
      "4532\n",
      "4534\n",
      "4548\n",
      "4645\n",
      "4814\n",
      "4855\n",
      "4986\n",
      "5034\n",
      "5041\n",
      "5062\n",
      "5063\n",
      "5121\n",
      "5133\n",
      "5146\n",
      "5188\n",
      "5205\n",
      "5214\n",
      "5256\n",
      "5258\n",
      "5286\n",
      "5292\n",
      "5332\n",
      "5379\n",
      "5399\n",
      "5433\n",
      "5588\n",
      "5798\n",
      "5846\n",
      "5884\n",
      "6032\n",
      "6113\n",
      "6253\n",
      "6260\n",
      "6272\n",
      "6306\n",
      "6329\n",
      "6493\n",
      "6498\n",
      "6575\n",
      "6650\n",
      "6674\n",
      "6839\n",
      "6857\n",
      "6873\n",
      "6893\n",
      "6927\n",
      "6952\n",
      "6965\n",
      "6984\n",
      "6993\n",
      "7035\n",
      "7123\n",
      "7154\n",
      "7174\n",
      "7188\n",
      "7203\n",
      "7212\n",
      "7298\n",
      "7344\n",
      "7451\n",
      "7494\n",
      "7500\n",
      "7539\n",
      "7603\n",
      "7608\n",
      "7704\n",
      "7739\n",
      "7751\n",
      "7790\n",
      "7818\n",
      "7846\n",
      "7887\n",
      "7968\n",
      "8008\n",
      "8027\n",
      "8029\n",
      "8078\n",
      "8086\n",
      "8127\n",
      "8143\n",
      "8158\n",
      "8181\n",
      "8195\n",
      "8245\n",
      "8254\n",
      "8562\n",
      "8598\n",
      "8628\n",
      "8648\n",
      "8674\n",
      "8688\n",
      "8697\n",
      "8701\n",
      "8705\n",
      "8718\n",
      "8754\n",
      "8760\n",
      "8779\n",
      "8851\n",
      "8920\n",
      "8995\n",
      "9047\n",
      "9080\n",
      "9086\n",
      "9093\n",
      "9099\n",
      "9102\n",
      "9212\n",
      "9323\n",
      "9327\n",
      "9451\n",
      "9555\n",
      "9568\n",
      "9658\n",
      "9711\n",
      "9748\n",
      "9768\n",
      "9834\n",
      "10001\n",
      "10026\n",
      "10157\n",
      "10215\n",
      "10229\n",
      "10240\n",
      "10277\n",
      "10306\n",
      "10314\n",
      "10559\n",
      "10567\n",
      "10753\n",
      "10800\n",
      "10843\n",
      "10938\n",
      "10990\n",
      "11038\n",
      "11058\n",
      "11059\n",
      "11138\n",
      "11177\n",
      "11354\n",
      "11509\n",
      "11533\n",
      "11571\n",
      "11653\n",
      "11682\n",
      "11763\n",
      "11772\n",
      "11799\n",
      "11853\n",
      "11860\n",
      "12005\n",
      "12016\n",
      "12033\n",
      "12141\n",
      "12154\n",
      "12292\n",
      "12307\n",
      "12317\n",
      "12333\n",
      "12341\n",
      "12401\n",
      "12422\n",
      "12493\n",
      "12677\n",
      "12859\n",
      "12866\n",
      "12980\n",
      "12999\n",
      "13060\n",
      "13063\n",
      "13114\n",
      "13158\n",
      "13238\n",
      "13254\n",
      "13391\n",
      "13413\n",
      "13432\n",
      "13643\n",
      "13653\n",
      "13738\n",
      "13743\n",
      "13755\n",
      "13772\n",
      "13775\n",
      "13788\n",
      "13908\n",
      "13940\n",
      "14005\n",
      "14042\n",
      "14167\n",
      "14278\n",
      "14282\n",
      "14421\n",
      "14457\n",
      "14655\n",
      "14667\n",
      "14745\n",
      "14794\n",
      "14807\n",
      "14850\n",
      "14921\n",
      "14954\n",
      "14973\n",
      "14994\n",
      "15070\n",
      "15082\n",
      "15230\n",
      "15258\n",
      "15412\n",
      "15424\n",
      "15503\n",
      "15554\n",
      "15730\n",
      "15862\n",
      "15868\n",
      "15885\n",
      "15908\n",
      "15962\n",
      "15965\n",
      "16133\n",
      "16319\n",
      "16361\n",
      "16440\n",
      "16529\n",
      "16531\n",
      "16547\n",
      "16614\n",
      "16619\n",
      "16681\n",
      "16819\n",
      "16932\n",
      "17099\n",
      "17254\n",
      "17289\n",
      "17292\n",
      "17327\n",
      "17353\n",
      "17398\n",
      "17442\n",
      "17499\n",
      "17500\n",
      "17525\n",
      "17548\n",
      "17595\n",
      "17601\n",
      "17602\n",
      "17607\n",
      "17642\n",
      "17703\n",
      "17736\n",
      "17799\n",
      "17820\n",
      "17827\n",
      "17951\n",
      "17970\n",
      "18019\n",
      "18040\n",
      "18053\n",
      "18076\n",
      "18102\n",
      "18119\n",
      "18150\n",
      "18191\n",
      "18285\n",
      "18327\n",
      "18337\n",
      "18339\n",
      "18378\n",
      "18441\n",
      "18510\n",
      "18514\n",
      "18599\n",
      "18640\n",
      "18785\n",
      "18823\n",
      "18880\n",
      "18886\n",
      "18998\n",
      "19028\n",
      "19032\n",
      "19043\n",
      "19062\n",
      "19171\n",
      "19174\n",
      "19200\n",
      "19231\n",
      "19371\n",
      "19393\n",
      "19421\n",
      "19435\n",
      "19528\n",
      "19631\n",
      "19698\n",
      "19738\n",
      "19796\n",
      "19869\n",
      "19877\n",
      "19963\n",
      "20215\n",
      "20242\n",
      "20287\n",
      "20300\n",
      "20440\n",
      "20509\n",
      "20540\n",
      "20624\n",
      "20711\n",
      "20712\n",
      "20733\n",
      "20761\n",
      "20814\n",
      "20877\n",
      "20891\n",
      "20905\n",
      "20936\n",
      "21016\n",
      "21115\n",
      "21132\n",
      "21187\n",
      "21258\n",
      "21348\n",
      "21387\n",
      "21401\n",
      "21450\n",
      "21517\n",
      "21565\n",
      "21592\n",
      "21654\n",
      "21670\n",
      "21710\n",
      "21834\n",
      "21868\n",
      "21912\n",
      "21935\n",
      "21939\n",
      "21946\n",
      "21992\n",
      "22046\n",
      "22111\n",
      "22170\n",
      "22287\n",
      "22297\n",
      "22337\n",
      "22344\n",
      "22404\n",
      "22434\n",
      "22516\n",
      "22535\n",
      "22590\n",
      "22633\n",
      "22811\n",
      "22857\n",
      "22959\n",
      "22960\n",
      "22977\n",
      "23028\n",
      "23065\n",
      "23101\n",
      "23184\n",
      "23350\n",
      "23453\n",
      "23459\n",
      "23500\n",
      "23517\n",
      "23528\n",
      "23577\n",
      "23620\n",
      "23676\n",
      "23716\n",
      "23866\n",
      "23893\n",
      "23894\n",
      "23906\n",
      "23912\n",
      "23926\n",
      "23981\n",
      "24092\n",
      "24110\n",
      "24134\n",
      "24138\n",
      "24268\n",
      "24291\n",
      "24298\n",
      "24325\n",
      "24416\n",
      "24468\n",
      "24507\n",
      "24569\n",
      "24606\n",
      "24636\n",
      "24691\n",
      "24741\n",
      "24759\n",
      "24882\n",
      "24930\n",
      "24968\n",
      "24984\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "record the positive and negative indices\n",
    "'''\n",
    "\n",
    "y_pos_indices = np.asarray(np.where(y_tr == 1)).reshape(12500)\n",
    "y_neg_indices = np.asarray(np.where(y_tr == 0)).reshape(12500)\n",
    "\n",
    "'''\n",
    "Make a random indices (0,12500) s.t it distribute among the corpus\n",
    "'''\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "rand = random.sample(range(0, 12500), 500)\n",
    "rand_indices = sorted(rand)\n",
    "for i in rand_indices:\n",
    "    print(y_neg_indices[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8279\n",
      "corpus index :  16583\n",
      "label :  1\n",
      "0 : i myself feel this film is a rare treasure.\n",
      "1 : not only is it the beginning of shirley temple's career, but a rare look on how our society has changed.\n",
      "2 : you have to understand, certain thing we today would view as sexual, back then would be considered innocent.\n",
      "3 : for example, the pare not of the child in the film as well as the many pare not who took their child to see this movie, saw this as just child mimicking adult.\n",
      "4 : most person did not think of anyone viewing child sexually attractive, other than teenage boy lusting over teenage girl.\n",
      "5 : to them it was not sexual.\n",
      "6 : mind you this was before we had internet, tv, etc... most sex crime were not openly brought up.\n",
      "7 : occasionally there would be a whisper about the kid with the \"funny uncle.\"\n",
      "8 : but that was often all that came of it.\n",
      "9 : yes very sad.\n",
      "10 : but it is kinda sad today, for even i too can see this film as anything other than what it was intended, innocent and funny.\n",
      "11 : when i saw shirley dance like that and the boy eye balling her, yes i felt disturbed.\n",
      "12 : i have to remind myself the time this took place!\n",
      "13 : those child did not know what sex was.\n",
      "14 : the pare not knew that, both those of the child in the movie and those watching it.\n",
      "15 : the thought may not have even entered their mind.\n",
      "16 : in the eye of the average adult back then, this was no more sexual then if shirley was playing house.\n",
      "17 : even today kid will enter beauty contest, many dressed up extremely maturely, for a three yr old.\n",
      "18 : however the child is merely pretending.\n",
      "19 : i do not blame the child for wanting to act like an adult.\n",
      "20 : or the old movie that display this.\n",
      "21 : in all honesty, our medium has made a lot of thing seem back then seem sick and wrong.\n",
      "22 : this sometimes can be for the best.\n",
      "23 : but i truly believe this movie is not one of them.\n",
      "24 : it gives a rare look of an innocent mentality, that we have long lost.\n"
     ]
    }
   ],
   "source": [
    "def print_sentence(corpus):\n",
    "    from textblob import TextBlob\n",
    "    text = TextBlob(corpus)\n",
    "    i = 0\n",
    "    for sentence in text.sentences:\n",
    "        print(i, ':', sentence)\n",
    "        i += 1\n",
    "\n",
    "        \n",
    "#### POSITIVE\n",
    "# 48\n",
    "NOW = 326\n",
    "print(rand_indices[NOW])\n",
    "print('corpus index : ', y_pos_indices[rand_indices[NOW]])\n",
    "print('label : ', y_tr[y_pos_indices[rand_indices[NOW]]])\n",
    "print_sentence(X_tr_clean[y_pos_indices[rand_indices[NOW]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1771\n",
      "corpus index :  3456\n",
      "label :  0\n",
      "0 : steven buddy, you remember when you said this: \"try to find the path of least resistance and use it without harming other.\n",
      "1 : live with integrity and morality, not only with person but with all being.\"\n",
      "2 : you have not been doing that, you have mortally wounded your fan and their morality with these \"film\" i would not even bother if i did not know you are so much better than this, i have seen the videos of you teaching, you are so much better than this why why brother why...steamroller production has been steamrolled i promise bro i am not afraid of you i will tell you the truth to your face so we can fix it.well i like some other fell asleep 90% in, but to be fair i was tired and had a large meal just an hour before hand sensai, what are you doing.\n",
      "3 : 12 million?\n",
      "4 : really?\n",
      "5 : do you have any idea what we could have done with $12,000,000 it could have been in the theater and a blockbuster hit, if you wanted we could have donated money from the huge profit to a homeless shelter or something.\n",
      "6 : these post production person are ripping you off man the choreography was non existent, we can do better man, the eye blinking thing was from the man in black movie, i half expected will smith to appear or tommy lee jone to tell your they were gill not eyelids.seagal you are an aikido master, why are you doing this to yourself, to us?\n",
      "7 : when you came on the scene, you had such a fresh direct style, and it was obvious you are a teacher cause the way your move were so clear and crisp, watching your first three movie i felt like you were teaching me something, now i feel like you are just being ripped off or something i feel like i need to save you buddy, this time you are the one who was killed and i am gonna go and get revenge for you by helping you make the best movie ever.\n",
      "8 : bro i know who you really are, i know the truth about the nico movie.\n",
      "9 : let's talk.contact me man i got some fresh idea i am a nit picker, i swear you will not be disappointed with my attention to detail and we will do it for the fan man, your fan deserve better, we are hanging on, but the strand is about to snap.\n",
      "10 : i swear i will not let your movie out the door with a single mistake in it i am still trying to figure out if that was the worst dubbing ever, or you have laryngitis, but i promise you i can do a better impression of your voice than the lame **** who did not even try.\n",
      "11 : i sure hope you kicked him in the nut as his paymant.\n",
      "12 : i can come up with a story and a plot that can be matched to your avenging the death of your student/daughter/wife/dog/house plant niche and i promise you we will bring you back, i promise, also i want to go in the direction, that makes person think, if you let me in i promise we will make a movie that person will walk away and have to have a discussion about it, a serious thought provoking, perception altering experience.steven seagal this is my official in writing permission for imdb to release my contact info to you for the purpose of resurrecting one of the best martial art hero i have ever seen also, for the record hes not italian, hes irish and jew so you call it bad acting i call it terrific acting, because you have believed for 20 year that seagal is italian :) kinda change your perception does not it.\n"
     ]
    }
   ],
   "source": [
    "### NEGATIVE\n",
    "\n",
    "NOW = 83\n",
    "print(rand_indices[NOW])\n",
    "print('corpus index : ', y_neg_indices[rand_indices[NOW]])\n",
    "print('label : ', y_tr[y_neg_indices[rand_indices[NOW]]])\n",
    "print_sentence(X_tr_clean[y_neg_indices[rand_indices[NOW]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good da. ad day\n"
     ]
    }
   ],
   "source": [
    "string = \"good day.bad day\"\n",
    "print(re.sub(re.compile('[a-z]\\.[a-z]'), '. ', string))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
