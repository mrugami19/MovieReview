{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############ Compulsory Standard Library #################\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "############ Sklearn pre-processing Library #################\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "############ Sklearn model Library #################\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_imdb(path):\n",
    "    \n",
    "    print(\"Loading the imdb data\")\n",
    "    \n",
    "    train_neg_files = glob.glob(path+\"/train/neg/*.txt\")\n",
    "    train_pos_files = glob.glob(path+\"/train/pos/*.txt\")\n",
    "    \n",
    "    X_train_corpus = []\n",
    "    y_train = []\n",
    "    \n",
    "    for tnf in train_neg_files:\n",
    "        f = open(tnf, 'r', encoding=\"utf8\")\n",
    "        line = f.read()\n",
    "        #line = line[:len(line)/2]\n",
    "        X_train_corpus.append(line)\n",
    "        y_train.append(0)\n",
    "        f.close()\n",
    "    \n",
    "    for tpf in train_pos_files:\n",
    "        f = open(tpf, 'r', encoding=\"utf8\")\n",
    "        line = f.read()\n",
    "        #line = line[:len(line)/2]\n",
    "        X_train_corpus.append(line)\n",
    "        y_train.append(1)\n",
    "        f.close()\n",
    "    \n",
    "    print(\"Train Data loaded.\")\n",
    "    \n",
    "    test_neg_files = glob.glob(path+\"/test/neg/*.txt\")\n",
    "    test_pos_files = glob.glob(path+\"/test/pos/*.txt\")\n",
    "    \n",
    "    X_test_corpus = []\n",
    "    y_test = []\n",
    "    \n",
    "    for tnf in test_neg_files:\n",
    "        f = open(tnf, 'r', encoding=\"utf8\")\n",
    "        X_test_corpus.append(f.read())\n",
    "        y_test.append(0)\n",
    "        f.close()\n",
    "    \n",
    "    for tpf in test_pos_files:\n",
    "        f = open(tpf, 'r', encoding=\"utf8\")\n",
    "        X_test_corpus.append(f.read())\n",
    "        y_test.append(1)\n",
    "        f.close()\n",
    "    \n",
    "    print(\"Test Data loaded.\")\n",
    "    \n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    return X_train_corpus, y_train, X_test_corpus , y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_contraction_list(filename):\n",
    "    vocabulary = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for l in f:\n",
    "            vocabulary.append(l.strip().split(','))\n",
    "    return np.asarray(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_contraction(corpus, cont_list):\n",
    "    for i in range(0, cont_list.shape[0]):\n",
    "        corpus = corpus.lower().replace(cont_list[i,0], cont_list[i,1])\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_corpus_contraction(X_corpus):\n",
    "    cont_list = load_contraction_list(\"contraction_list.txt\")\n",
    "\n",
    "    print('corpus update start')\n",
    "    for i in range(0,len(X_corpus)):\n",
    "        X_corpus[i] = cleanhtml(X_corpus[i])\n",
    "        X_corpus[i] = replace_contraction(X_corpus[i], cont_list)\n",
    "    print('corpus update end')\n",
    "    print()\n",
    "    return X_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negative_positive_counts(X, y, word_index):\n",
    "    neg_count = np.sum(X[y==0, word_index])\n",
    "    pos_count = np.sum(X[y==1, word_index])    \n",
    "    return neg_count, pos_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_top_words_with_count(X, y, words, weights, filename, top_k=10):\n",
    "    sorted_indices_descending_abs = np.argsort(np.absolute(weights))[::-1]\n",
    "    \n",
    "    filename = filename + '.txt'\n",
    "    with open(filename, mode='w', encoding='utf8') as w:\n",
    "        for i in sorted_indices_descending_abs[: top_k]:\n",
    "#             print(\"%s\\t%0.2f\" %(words[i], weights[i]))\n",
    "            n_p=negative_positive_counts(X, y, i)\n",
    "            w.write(\"%s\\t%0.2f\\t%d\\t%d\" %(str(words[i]), weights[i], n_p[0], n_p[1]))\n",
    "            w.write('\\n')\n",
    "        w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanhtml(text):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantag = re.sub(cleanr, '', text)\n",
    "    cleantext = cleantag.replace('br', '')\n",
    "    return cleantext\n",
    "\n",
    "# Reference :\n",
    "# https://stackoverflow.com/questions/9662346/python-code-to-remove-html-tags-from-a-string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the imdb data\n",
      "Train Data loaded.\n",
      "Test Data loaded.\n"
     ]
    }
   ],
   "source": [
    "X_train_corpus , y_train, X_test_corpus , y_test = load_imdb('../aclImdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Update (Remove the contraction)\n",
    "Such as :<br>\n",
    "[isn't $\\rightarrow$ is not] <br>\n",
    "[haven't $\\rightarrow$ have not] <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus update start\n",
      "corpus update end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_corpus_update = update_corpus_contraction(X_train_corpus)\n",
    "# X_test_corpus_update = update_corpus_contraction(X_test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top 100 words extraction (1-5 grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token = r\"(?u)\\b[\\w\\'/]+\\b\"\n",
    "tf_vectorizer = CountVectorizer(lowercase=True, max_df=1.0, min_df=5, binary=True, token_pattern=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 grams\n",
      "Processing 2 grams\n",
      "Processing 3 grams\n",
      "Processing 4 grams\n",
      "Processing 5 grams\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    print('Processing',i,'grams')\n",
    "    tf_vectorizer.set_params(ngram_range=(i,i))\n",
    "    X_train = tf_vectorizer.fit_transform(X_train_corpus_update)\n",
    "    words = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "    clf = LogisticRegression(penalty='l2', C=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    filename=str(i)+\"gram\"\n",
    "    sort_top_words_with_count(X_train, y_train, words, clf.coef_.flatten(), filename, top_k=100)\n",
    "    \n",
    "    del clf\n",
    "    del X_train\n",
    "    del words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
