{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############ Compulsory Standard Library #################\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "############ Sklearn pre-processing Library #################\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "############ Sklearn model Library #################\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_imdb(path):\n",
    "    \n",
    "    print(\"Loading the imdb data\")\n",
    "    \n",
    "    train_neg_files = glob.glob(path+\"/train/neg/*.txt\")\n",
    "    train_pos_files = glob.glob(path+\"/train/pos/*.txt\")\n",
    "    \n",
    "    X_train_corpus = []\n",
    "    y_train = []\n",
    "    \n",
    "    for tnf in train_neg_files:\n",
    "        f = open(tnf, 'r', encoding=\"utf8\")\n",
    "        line = f.read()\n",
    "        #line = line[:len(line)/2]\n",
    "        X_train_corpus.append(line)\n",
    "        y_train.append(0)\n",
    "        f.close()\n",
    "    \n",
    "    for tpf in train_pos_files:\n",
    "        f = open(tpf, 'r', encoding=\"utf8\")\n",
    "        line = f.read()\n",
    "        #line = line[:len(line)/2]\n",
    "        X_train_corpus.append(line)\n",
    "        y_train.append(1)\n",
    "        f.close()\n",
    "    \n",
    "    print(\"Train Data loaded.\")\n",
    "    \n",
    "    test_neg_files = glob.glob(path+\"/test/neg/*.txt\")\n",
    "    test_pos_files = glob.glob(path+\"/test/pos/*.txt\")\n",
    "    \n",
    "    X_test_corpus = []\n",
    "    y_test = []\n",
    "    \n",
    "    for tnf in test_neg_files:\n",
    "        f = open(tnf, 'r', encoding=\"utf8\")\n",
    "        X_test_corpus.append(f.read())\n",
    "        y_test.append(0)\n",
    "        f.close()\n",
    "    \n",
    "    for tpf in test_pos_files:\n",
    "        f = open(tpf, 'r', encoding=\"utf8\")\n",
    "        X_test_corpus.append(f.read())\n",
    "        y_test.append(1)\n",
    "        f.close()\n",
    "    \n",
    "    print(\"Test Data loaded.\")\n",
    "    \n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    return X_train_corpus, y_train, X_test_corpus , y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_list(filename, split_delimiter):\n",
    "    vocabulary = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for l in f:\n",
    "            vocabulary.append(l.strip().split(split_delimiter))\n",
    "    return np.asarray(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_contraction(corpus, cont_list):\n",
    "    for i in range(0, cont_list.shape[0]):\n",
    "        corpus = corpus.lower().replace(cont_list[i,0], cont_list[i,1])\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_corpus_contraction(X_corpus):\n",
    "    cont_list = load_list(\"contraction_list.txt\", ',')\n",
    "\n",
    "    print('corpus update start')\n",
    "    for i in range(0,len(X_corpus)):\n",
    "        X_corpus[i] = cleanhtml(X_corpus[i])\n",
    "        X_corpus[i] = replace_contraction(X_corpus[i], cont_list)\n",
    "    print('corpus update end')\n",
    "    print()\n",
    "    return X_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negative_positive_counts(X, y, word_index):\n",
    "    neg_count = np.sum(X[y==0, word_index])\n",
    "    pos_count = np.sum(X[y==1, word_index])    \n",
    "    return neg_count, pos_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_top_words_with_count(X, y, words, weights, filename, top_k=10):\n",
    "    sorted_indices_descending_abs = np.argsort(np.absolute(weights))[::-1]\n",
    "    \n",
    "    filename = filename + '.txt'\n",
    "    with open(filename, mode='w', encoding='utf8') as w:\n",
    "        for i in sorted_indices_descending_abs[: top_k]:\n",
    "#             print(\"%s\\t%0.2f\" %(words[i], weights[i]))\n",
    "            n_p=negative_positive_counts(X, y, i)\n",
    "            w.write(\"%s\\t%0.2f\\t%d\\t%d\" %(str(words[i]), weights[i], n_p[0], n_p[1]))\n",
    "            w.write('\\n')\n",
    "        w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanhtml(text):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantag = re.sub(cleanr, '', text)\n",
    "    cleantext = cleantag.replace('br', '')\n",
    "    return cleantext\n",
    "\n",
    "# Reference :\n",
    "# https://stackoverflow.com/questions/9662346/python-code-to-remove-html-tags-from-a-string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the imdb data\n",
      "Train Data loaded.\n",
      "Test Data loaded.\n"
     ]
    }
   ],
   "source": [
    "X_train_corpus , y_train, X_test_corpus , y_test = load_imdb('../aclImdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Update (Remove the contraction)\n",
    "Such as :<br>\n",
    "[isn't $\\rightarrow$ is not] <br>\n",
    "[haven't $\\rightarrow$ have not] <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus update start\n",
      "corpus update end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_corpus_update = update_corpus_contraction(X_train_corpus)\n",
    "# X_test_corpus_update = update_corpus_contraction(X_test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top 100 words extraction (1-5 grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token = r\"(?u)\\b[\\w\\'/]+\\b\"\n",
    "tf_vectorizer = CountVectorizer(lowercase=True, max_df=1.0, min_df=5, binary=True, token_pattern=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 grams\n",
      "Processing 2 grams\n",
      "Processing 3 grams\n",
      "Processing 4 grams\n",
      "Processing 5 grams\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    print('Processing',i,'grams')\n",
    "    tf_vectorizer.set_params(ngram_range=(i,i))\n",
    "    X_train = tf_vectorizer.fit_transform(X_train_corpus_update)\n",
    "    words = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "    clf = LogisticRegression(penalty='l2', C=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    filename=str(i)+\"gram\"\n",
    "    sort_top_words_with_count(X_train, y_train, words, clf.coef_.flatten(), filename, top_k=100)\n",
    "    \n",
    "    del clf\n",
    "    del X_train\n",
    "    del words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate tree relation\n",
    "\n",
    "Reference :\n",
    "https://stackoverflow.com/questions/2358045/how-can-i-implement-a-tree-in-python-are-there-any-built-in-data-structures-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_gram = load_list(\"1gram.txt\",'\\t')\n",
    "two_gram = load_list(\"2gram.txt\",'\\t')\n",
    "three_gram = load_list(\"3gram.txt\", '\\t')\n",
    "four_gram = load_list(\"4gram.txt\", '\\t')\n",
    "five_gram = load_list(\"5gram.txt\", '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst -2.37 \t the worst -2.52\n",
      "wonder -1.06 \t is wonderful 0.84\n",
      "wonder -1.06 \t a wonderful 0.81\n",
      "highly 1.03 \t highly recommended 1.27\n"
     ]
    }
   ],
   "source": [
    "for i in one_gram:\n",
    "    for j in two_gram:\n",
    "#         abs_weight = [np.absolute(j[1]), np.absolute(i[1])]\n",
    "        if i[0] in j[0] and (j[1]>i[1]):\n",
    "            print(i[0], i[1],'\\t', j[0], j[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the worst -2.52 \t of the worst -2.80\n",
      "waste of -1.54 \t a waste of -1.68\n",
      "than this -1.23 \t better than this -1.37\n",
      "a must 1.09 \t a must see 1.45\n",
      "worst movie -1.08 \t the worst movie -1.37\n",
      "an excellent 1.05 \t is an excellent 1.06\n",
      "loved this 1.04 \t i loved this 1.40\n",
      "unless you -1.04 \t unless you are -1.36\n",
      "the best 1.01 \t of the best 1.54\n",
      "love this 0.95 \t i love this 1.51\n",
      "not even -0.94 \t is not even -1.23\n",
      "not even -0.94 \t does not even -0.95\n",
      "very bad -0.92 \t a very bad -1.07\n",
      "very good 0.90 \t very good and 0.92\n",
      "so bad -0.90 \t is so bad -1.10\n",
      "so bad -0.90 \t was so bad -0.91\n",
      "sit through -0.90 \t to sit through -1.23\n",
      "a great 0.89 \t a great job 1.20\n",
      "a great 0.89 \t is a great 1.10\n",
      "a great 0.89 \t with a great 0.97\n",
      "your time -0.88 \t waste your time -1.05\n",
      "the funniest 0.88 \t of the funniest 0.93\n",
      "must see 0.87 \t a must see 1.45\n",
      "must see 0.87 \t must see for 0.91\n",
      "very disappointed -0.87 \t was very disappointed -0.93\n",
      "at all -0.86 \t at all costs -1.20\n",
      "highly recommend 0.86 \t i highly recommend 1.13\n",
      "highly recommend 0.86 \t highly recommend this 0.92\n",
      "enjoyed it 0.85 \t i enjoyed it 1.23\n",
      "skip this -0.85 \t skip this one -0.97\n",
      "a true 0.83 \t a true story 0.98\n",
      "great job 0.83 \t a great job 1.20\n",
      "a terrible -0.83 \t a terrible movie -1.07\n",
      "your money -0.82 \t save your money -1.06\n",
      "none of -0.82 \t none of the -1.16\n",
      "nothing more -0.80 \t nothing more than -0.98\n",
      "nothing more -0.80 \t is nothing more -0.90\n",
      "just does -0.79 \t just does not -1.04\n",
      "not miss 0.79 \t do not miss 0.89\n"
     ]
    }
   ],
   "source": [
    "for i in two_gram:\n",
    "    for j in three_gram:\n",
    "        if i[0] in j[0] and j[1]>i[1]:\n",
    "            print(i[0], i[1],'\\t', j[0], j[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of the worst -2.80 \t one of the worst -2.95\n",
      "a waste of -1.68 \t a waste of time -1.81\n",
      "is the worst -1.57 \t this is the worst -1.73\n",
      "waste of time -1.54 \t a waste of time -1.81\n",
      "of the best 1.54 \t one of the best 1.81\n",
      "i love this 1.51 \t i love this movie 1.81\n",
      "a must see 1.45 \t a must see for 1.92\n",
      "i loved this 1.40 \t i loved this movie 1.61\n",
      "is not even -1.23 \t it is not even -1.42\n",
      "to sit through -1.23 \t to sit through this -1.36\n",
      "i recommend this 1.22 \t i recommend this movie 1.33\n",
      "at all costs -1.20 \t avoid at all costs -1.77\n",
      "a great job 1.20 \t does a great job 1.66\n",
      "none of the -1.16 \t none of the characters -1.30\n",
      "the only good -1.15 \t the only good thing -1.66\n",
      "i highly recommend 1.13 \t i highly recommend this 1.59\n",
      "i highly recommend 1.13 \t i highly recommend it 1.23\n",
      "that is it -1.12 \t and that is it -1.42\n",
      "is a great 1.10 \t this is a great 1.66\n",
      "a very bad -1.07 \t is a very bad -1.13\n",
      "is an excellent 1.06 \t this is an excellent 1.52\n",
      "save your money -1.06 \t save your money and -1.27\n",
      "is definitely worth 1.04 \t it is definitely worth 1.36\n",
      "one of my 1.03 \t one of my favorite 1.40\n",
      "i am sorry -1.01 \t i am sorry but -1.26\n",
      "nothing more than -0.98 \t is nothing more than -1.31\n",
      "is the best 0.96 \t this is the best 1.21\n",
      "the most boring -0.94 \t of the most boring -1.44\n",
      "love this movie 0.94 \t i love this movie 1.81\n",
      "not waste your -0.94 \t do not waste your -1.82\n",
      "was very disappointed -0.93 \t i was very disappointed -1.37\n",
      "is well worth 0.92 \t it is well worth 1.24\n",
      "highly recommend this 0.92 \t i highly recommend this 1.59\n",
      "was so bad -0.91 \t it was so bad -1.22\n",
      "must see for 0.91 \t a must see for 1.92\n",
      "do not waste -0.91 \t do not waste your -1.82\n",
      "tries to be -0.91 \t it tries to be -1.24\n",
      "this piece of -0.90 \t this piece of crap -1.34\n",
      "this piece of -0.90 \t this piece of garbage -1.25\n",
      "is nothing more -0.90 \t is nothing more than -1.31\n",
      "not watch this -0.89 \t do not watch this -1.51\n",
      "mystery science theater -0.89 \t mystery science theater 3000 -1.60\n",
      "do not miss 0.89 \t do not miss it 1.64\n",
      "do not miss 0.89 \t do not miss this 1.33\n"
     ]
    }
   ],
   "source": [
    "for i in three_gram:\n",
    "    for j in four_gram:\n",
    "        if i[0] in j[0] and j[1]>i[1]:\n",
    "            print(i[0], i[1],'\\t', j[0], j[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do not waste your -1.82 \t do not waste your time -2.36\n",
      "do not waste your -1.82 \t do not waste your money -2.01\n",
      "i really enjoyed this 1.72 \t i really enjoyed this movie 2.07\n",
      "i highly recommend this 1.59 \t i highly recommend this film 1.60\n",
      "of the most boring -1.44 \t one of the most boring -1.98\n",
      "it is not even -1.42 \t and it is not even -1.54\n",
      "this is a must 1.36 \t this is a must see 1.94\n",
      "do not miss this 1.33 \t do not miss this one 1.80\n",
      "is nothing more than -1.31 \t is nothing more than a -1.78\n",
      "of the most awful -1.30 \t one of the most awful -1.60\n",
      "worst film i have -1.26 \t the worst film i have -1.56\n",
      "a lot of fun 1.25 \t is a lot of fun 1.44\n",
      "complete waste of time -1.23 \t a complete waste of time -1.79\n",
      "the worst i have -1.20 \t the worst i have seen -1.63\n",
      "this is a wonderful 1.18 \t this is a wonderful film 1.37\n",
      "a total waste of -1.15 \t a total waste of time -1.66\n",
      "fell in love with 1.13 \t i fell in love with 1.59\n",
      "one of the greatest 1.13 \t is one of the greatest 1.40\n"
     ]
    }
   ],
   "source": [
    "for i in four_gram:\n",
    "    for j in five_gram:\n",
    "        if i[0] in j[0] and j[1]>i[1]:\n",
    "            print(i[0], i[1],'\\t', j[0], j[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
