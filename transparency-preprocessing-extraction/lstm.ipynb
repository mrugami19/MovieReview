{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential  \n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "def load_list(filename):\n",
    "    vocabulary = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for l in f:\n",
    "            vocabulary.append(l.strip())\n",
    "    return np.asarray(vocabulary)\n",
    "\n",
    "pos_related = load_list('./sentence_data/pos_related.txt')\n",
    "neg_related = load_list('./sentence_data/neg_related.txt')\n",
    "pos_unrelated = load_list('./sentence_data/pos_unrelated.txt')\n",
    "neg_unrelated = load_list('./sentence_data/neg_unrelated.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(466,)\n",
      "(83,)\n",
      "(388,)\n",
      "(34,)\n"
     ]
    }
   ],
   "source": [
    "print(pos_related.shape)\n",
    "print(neg_related.shape)\n",
    "print(pos_unrelated.shape)\n",
    "print(neg_unrelated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(549,)\n",
      "(549,)\n",
      "(422,)\n",
      "(422,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(971,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack the data\n",
    "\n",
    "related_set = np.hstack((pos_related, neg_related))\n",
    "print(related_set.shape)\n",
    "y_related = np.ones(related_set.shape)\n",
    "print(y_related.shape)\n",
    "\n",
    "unrelated_set = np.hstack((pos_unrelated, neg_unrelated))\n",
    "print(unrelated_set.shape)\n",
    "y_unrelated = np.zeros(unrelated_set.shape)\n",
    "print(y_unrelated.shape)\n",
    "\n",
    "X_stack = np.hstack((related_set, unrelated_set))\n",
    "y_stack = np.hstack((y_related, y_unrelated))\n",
    "\n",
    "X_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "token = r\"(?u)\\b[\\w\\'/]+\\b\"\n",
    "tf_vectorizer = CountVectorizer(lowercase=True, max_df=1.0, min_df=1, binary=False, token_pattern=token)\n",
    "tf_vectorizer.set_params(ngram_range=(1,1))\n",
    "\n",
    "X_vectorized = tf_vectorizer.fit_transform(X_stack)\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y_stack, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = len(tf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The X should be 3d matrix, each instance is a 2d. \n",
    "# We are only using one hot encoder at each time t. \n",
    "# It also could be done using embedding or word2vec. \n",
    "\n",
    "# e = Embedding(voc, 32, input_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    }
   ],
   "source": [
    "max_length = 0\n",
    "for i in X_stack:\n",
    "    token = i.split(\" \")\n",
    "    if max_length < len(token):\n",
    "        max_length = len(token)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_neurons = voc \n",
    "\n",
    "hidden_neurons = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 10)                147880    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 147,891\n",
      "Trainable params: 147,891\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\anne soraya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "c:\\users\\anne soraya\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(input_shape=(None, 368..., units=10)`\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "# model.add(Embedding(voc, 32, input_length=max_length))\n",
    "# model.add(Flatten())\n",
    "model.add(LSTM(output_dim=hidden_neurons, input_dim=in_neurons))\n",
    "\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'], class_mode=\"binary\")\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
